<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  <link rel="shortcut icon" href="../img/favicon.ico">
  <title>Task - RIO ERFIAN 180411100040</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
  
  <script>
    // Current page data
    var mkdocs_page_name = "Task";
    var mkdocs_page_input_path = "task.md";
    var mkdocs_page_url = null;
  </script>
  
  <script src="../js/jquery-2.1.1.min.js" defer></script>
  <script src="../js/modernizr-2.8.3.min.js" defer></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href=".." class="icon icon-home"> RIO ERFIAN 180411100040</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
	  
          
            <li class="toctree-l1">
		
    <a class="" href="..">Home</a>
	    </li>
          
            <li class="toctree-l1 current">
		
    <a class="current" href="./">Task</a>
    <ul class="subnav">
            
    <li class="toctree-l2"><a href="#pusat-tugas-penambangan-data">Pusat Tugas Penambangan Data</a></li>
    
        <ul>
        
            <li><a class="toctree-l3" href="#tugas-10">TUGAS 1.0</a></li>
        
            <li><a class="toctree-l3" href="#tugas-20">TUGAS 2.0</a></li>
        
            <li><a class="toctree-l3" href="#tugas-30">TUGAS 3.0</a></li>
        
            <li><a class="toctree-l3" href="#tugas-40">TUGAS 4.0</a></li>
        
            <li><a class="toctree-l3" href="#tugas-50">TUGAS 5.0</a></li>
        
        </ul>
    

    </ul>
	    </li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="..">RIO ERFIAN 180411100040</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="..">Docs</a> &raquo;</li>
    
      
    
    <li>Task</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="pusat-tugas-penambangan-data">Pusat Tugas Penambangan Data</h1>
<p>For full documentation visit <a href="https://github.com/erfiandanuri80/Asli_PENAMBANGAN_DATA/">github.com/erfiandanuri80/Asli_PENAMBANGAN_DATA</a>.</p>
<div class="toc">
<ul>
<li><a href="#pusat-tugas-penambangan-data">Pusat Tugas Penambangan Data</a><ul>
<li><a href="#tugas-10"><u>TUGAS 1.0</u></a><ul>
<li><a href="#statistika-deskriptif">Statistika Deskriptif</a></li>
<li><a href="#manfaat-dari-statistik-deskriptif">Manfaat dari statistik deskriptif</a></li>
<li><a href="#ukuran-statistik-deskriptif">Ukuran statistik deskriptif</a><ul>
<li><a href="#1-ukuran-pemusatan">1. Ukuran pemusatan</a></li>
<li><a href="#2-ukuran-keragaman">2. Ukuran keragaman</a></li>
</ul>
</li>
<li><a href="#code">Code</a></li>
</ul>
</li>
<li><a href="#tugas-20"><u>TUGAS 2.0</u></a><ul>
<li><a href="#mengukur-jarak-data">Mengukur Jarak Data</a></li>
<li><a href="#mengukur-jarak-tipe-numerik">Mengukur Jarak Tipe Numerik</a></li>
<li><a href="#menghitung-jarak-ordinal">Menghitung Jarak Ordinal</a></li>
<li><a href="#menghitung-jarak-binary">Menghitung jarak Binary</a></li>
<li><a href="#menghitung-jarak-campuran">Menghitung Jarak Campuran</a></li>
<li><a href="#display">display</a></li>
</ul>
</li>
<li><a href="#tugas-30"><u>TUGAS 3.0</u></a><ul>
<li><a href="#seleksi-fitur">Seleksi Fitur</a></li>
<li><a href="#mencari-entropy">Mencari Entropy</a></li>
<li><a href="#gain">Gain</a></li>
<li><a href="#skor-keseluruhan-gain">Skor Keseluruhan Gain</a></li>
</ul>
</li>
<li><a href="#tugas-40"><u>TUGAS 4.0</u></a><ul>
<li><a href="#naive-bayes-classifiers">Naive Bayes Classifiers</a><ul>
<li><a href="#alur-kerja-klasifikasi">Alur Kerja Klasifikasi</a></li>
<li><a href="#teorema-bayes">Teorema Bayes</a></li>
<li><a href="#pendekatan-pertama-dalam-hal-fitur-tunggal">Pendekatan Pertama (Dalam hal fitur tunggal)</a></li>
<li><a href="#pendekatan-kedua-dalam-hal-banyak-fitur">Pendekatan Kedua (Dalam hal banyak fitur)</a></li>
</ul>
</li>
<li><a href="#bangunan-classifier-di-scikit-learn">Bangunan Classifier di Scikit-learn</a><ul>
<li><a href="#klasifikasi-naif-bayes">Klasifikasi Naif Bayes</a></li>
<li><a href="#mendefinisikan-dataset">Mendefinisikan Dataset</a></li>
<li><a href="#memuat-data">Memuat data</a></li>
<li><a href="#menjelajahi-data">Menjelajahi Data</a></li>
</ul>
</li>
<li><a href="#memisahkan-data">Memisahkan Data</a></li>
<li><a href="#pembuatan-model">Pembuatan Model</a></li>
<li><a href="#mengevaluasi-model">Mengevaluasi Model</a><ul>
<li><a href="#keuntungan">Keuntungan</a></li>
<li><a href="#kekurangan">Kekurangan</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#tugas-50"><u>TUGAS 5.0</u></a><ul>
<li><a href="#w-knn-weighted-k-nearest-neighbour">W-KNN (Weighted K-Nearest Neighbour)</a><ul>
<li><a href="#intuisi">Intuisi:</a></li>
<li><a href="#algoritma">Algoritma</a></li>
<li><a href="#implementasi">Implementasi:</a></li>
</ul>
</li>
<li><a href="#w-knn-dengan-dataset-iris">W-KNN dengan Dataset Iris</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<hr />
<h3 id="tugas-10"><u><strong>TUGAS 1.0</strong></u></h3>
<h5 id="statistika-deskriptif">Statistika Deskriptif</h5>
<p>Statistik deskriptif merupakan proses analisis statistik yang fokus kepada manejemen,   penyajian, dan klasifikasi data. Dengan proses ini, data yang disajikan akan menjadi lebih menarik lebih mudah dipahami, dan mampu memberikan makna lebih bagi pengguna data. Ada beberapa bentuk media yang biasa digunakan sebagai analisis deksriptif, diantaranya adalah tabel, grafik, diagram, infografis, dll. Tabel dan grafik adalah media yang biasa kita gunakan dalam menyajikan statistik deskriptif.</p>
<h5 id="manfaat-dari-statistik-deskriptif">Manfaat dari statistik deskriptif</h5>
<ol>
<li><strong>Memberikan gambaran dan deskripsi bagaimana informasi yang dimiliki data tersebut</strong></li>
</ol>
<p>Statistik deskriptif haruslah mampu memberikan gambaran informasi apa saja yang bisa didapat secara dari data yang kita gunakan. Daripada hanya menggunakan angka-angka tanpa format yang baku, akan lebih menarik bila ditampilan dalam bentuk grafik dan tabel.</p>
<ol>
<li><strong>Menjelaskan karakteristik sebuah data</strong></li>
</ol>
<p>Statistik deskriptif juga memberikan karakteristik tentang data yang digunakan. Hal ini penting karena kondisi data yang digunakan akan memengaruhi seluruh analisis data yang kita lakukan.</p>
<p><em>download Book1.csv terdahulu</em><a href="https://drive.google.com/file/d/1NkRYx_Pw4CMtbB3xzS6Wav9KRPVFuawe/view?usp=sharing"> disini</a></p>
<pre><code>import pandas as pd
from scipy import stats
d=pd.read_csv('Book1.csv')
</code></pre>

<table>
<thead>
<tr>
<th align="right"></th>
<th align="right">beratbadan</th>
<th align="right">tinggibadan</th>
<th align="right">umur</th>
<th>banyak saudara</th>
</tr>
</thead>
<tbody>
<tr>
<td align="right">0</td>
<td align="right">67</td>
<td align="right">157</td>
<td align="right">22</td>
<td>5</td>
</tr>
<tr>
<td align="right">1</td>
<td align="right">76</td>
<td align="right">165</td>
<td align="right">21</td>
<td>2</td>
</tr>
<tr>
<td align="right">2</td>
<td align="right">84</td>
<td align="right">156</td>
<td align="right">22</td>
<td>2</td>
</tr>
<tr>
<td align="right">3</td>
<td align="right">51</td>
<td align="right">167</td>
<td align="right">22</td>
<td>2</td>
</tr>
<tr>
<td align="right">4</td>
<td align="right">93</td>
<td align="right">192</td>
<td align="right">21</td>
<td>5</td>
</tr>
<tr>
<td align="right">5</td>
<td align="right">90</td>
<td align="right">171</td>
<td align="right">30</td>
<td>4</td>
</tr>
<tr>
<td align="right">6</td>
<td align="right">93</td>
<td align="right">187</td>
<td align="right">16</td>
<td>2</td>
</tr>
<tr>
<td align="right">7</td>
<td align="right">77</td>
<td align="right">163</td>
<td align="right">26</td>
<td>4</td>
</tr>
<tr>
<td align="right">8</td>
<td align="right">71</td>
<td align="right">153</td>
<td align="right">28</td>
<td>3</td>
</tr>
<tr>
<td align="right">9</td>
<td align="right">54</td>
<td align="right">167</td>
<td align="right">18</td>
<td>5</td>
</tr>
<tr>
<td align="right">10</td>
<td align="right">78</td>
<td align="right">196</td>
<td align="right">18</td>
<td>5</td>
</tr>
<tr>
<td align="right">11</td>
<td align="right">85</td>
<td align="right">159</td>
<td align="right">22</td>
<td>4</td>
</tr>
<tr>
<td align="right">12</td>
<td align="right">72</td>
<td align="right">192</td>
<td align="right">25</td>
<td>3</td>
</tr>
<tr>
<td align="right">13</td>
<td align="right">88</td>
<td align="right">164</td>
<td align="right">20</td>
<td>3</td>
</tr>
<tr>
<td align="right">14</td>
<td align="right">87</td>
<td align="right">172</td>
<td align="right">25</td>
<td>3</td>
</tr>
<tr>
<td align="right">15</td>
<td align="right">86</td>
<td align="right">198</td>
<td align="right">28</td>
<td>3</td>
</tr>
<tr>
<td align="right">16</td>
<td align="right">91</td>
<td align="right">177</td>
<td align="right">29</td>
<td>1</td>
</tr>
<tr>
<td align="right">17</td>
<td align="right">66</td>
<td align="right">174</td>
<td align="right">30</td>
<td>4</td>
</tr>
<tr>
<td align="right">18</td>
<td align="right">72</td>
<td align="right">164</td>
<td align="right">19</td>
<td>1</td>
</tr>
<tr>
<td align="right">19</td>
<td align="right">70</td>
<td align="right">183</td>
<td align="right">16</td>
<td>4</td>
</tr>
<tr>
<td align="right">20</td>
<td align="right">86</td>
<td align="right">170</td>
<td align="right">25</td>
<td>2</td>
</tr>
<tr>
<td align="right">21</td>
<td align="right">50</td>
<td align="right">199</td>
<td align="right">26</td>
<td>1</td>
</tr>
<tr>
<td align="right">22</td>
<td align="right">59</td>
<td align="right">185</td>
<td align="right">30</td>
<td>2</td>
</tr>
<tr>
<td align="right">23</td>
<td align="right">69</td>
<td align="right">198</td>
<td align="right">18</td>
<td>5</td>
</tr>
<tr>
<td align="right">24</td>
<td align="right">58</td>
<td align="right">184</td>
<td align="right">26</td>
<td>4</td>
</tr>
<tr>
<td align="right">25</td>
<td align="right">62</td>
<td align="right">168</td>
<td align="right">19</td>
<td>5</td>
</tr>
<tr>
<td align="right">26</td>
<td align="right">54</td>
<td align="right">188</td>
<td align="right">24</td>
<td>3</td>
</tr>
<tr>
<td align="right">27</td>
<td align="right">90</td>
<td align="right">187</td>
<td align="right">29</td>
<td>1</td>
</tr>
<tr>
<td align="right">28</td>
<td align="right">63</td>
<td align="right">197</td>
<td align="right">19</td>
<td>3</td>
</tr>
<tr>
<td align="right">29</td>
<td align="right">85</td>
<td align="right">172</td>
<td align="right">23</td>
<td>2</td>
</tr>
<tr>
<td align="right">...</td>
<td align="right">...</td>
<td align="right">...</td>
<td align="right">...</td>
<td>...</td>
</tr>
<tr>
<td align="right">70</td>
<td align="right">67</td>
<td align="right">159</td>
<td align="right">21</td>
<td>3</td>
</tr>
<tr>
<td align="right">71</td>
<td align="right">68</td>
<td align="right">173</td>
<td align="right">23</td>
<td>1</td>
</tr>
<tr>
<td align="right">72</td>
<td align="right">66</td>
<td align="right">154</td>
<td align="right">23</td>
<td>2</td>
</tr>
<tr>
<td align="right">73</td>
<td align="right">99</td>
<td align="right">177</td>
<td align="right">28</td>
<td>2</td>
</tr>
<tr>
<td align="right">74</td>
<td align="right">59</td>
<td align="right">197</td>
<td align="right">20</td>
<td>5</td>
</tr>
<tr>
<td align="right">75</td>
<td align="right">84</td>
<td align="right">183</td>
<td align="right">22</td>
<td>4</td>
</tr>
<tr>
<td align="right">76</td>
<td align="right">68</td>
<td align="right">184</td>
<td align="right">17</td>
<td>2</td>
</tr>
<tr>
<td align="right">77</td>
<td align="right">95</td>
<td align="right">195</td>
<td align="right">17</td>
<td>2</td>
</tr>
<tr>
<td align="right">78</td>
<td align="right">91</td>
<td align="right">157</td>
<td align="right">28</td>
<td>5</td>
</tr>
<tr>
<td align="right">79</td>
<td align="right">87</td>
<td align="right">194</td>
<td align="right">16</td>
<td>4</td>
</tr>
<tr>
<td align="right">80</td>
<td align="right">62</td>
<td align="right">194</td>
<td align="right">20</td>
<td>2</td>
</tr>
<tr>
<td align="right">81</td>
<td align="right">81</td>
<td align="right">186</td>
<td align="right">27</td>
<td>4</td>
</tr>
<tr>
<td align="right">82</td>
<td align="right">69</td>
<td align="right">175</td>
<td align="right">29</td>
<td>2</td>
</tr>
<tr>
<td align="right">83</td>
<td align="right">91</td>
<td align="right">174</td>
<td align="right">29</td>
<td>4</td>
</tr>
<tr>
<td align="right">84</td>
<td align="right">65</td>
<td align="right">178</td>
<td align="right">21</td>
<td>2</td>
</tr>
<tr>
<td align="right">85</td>
<td align="right">61</td>
<td align="right">156</td>
<td align="right">19</td>
<td>1</td>
</tr>
<tr>
<td align="right">86</td>
<td align="right">83</td>
<td align="right">156</td>
<td align="right">19</td>
<td>4</td>
</tr>
<tr>
<td align="right">87</td>
<td align="right">93</td>
<td align="right">185</td>
<td align="right">18</td>
<td>4</td>
</tr>
<tr>
<td align="right">88</td>
<td align="right">63</td>
<td align="right">187</td>
<td align="right">28</td>
<td>5</td>
</tr>
<tr>
<td align="right">89</td>
<td align="right">79</td>
<td align="right">184</td>
<td align="right">17</td>
<td>5</td>
</tr>
<tr>
<td align="right">90</td>
<td align="right">86</td>
<td align="right">157</td>
<td align="right">23</td>
<td>5</td>
</tr>
<tr>
<td align="right">91</td>
<td align="right">86</td>
<td align="right">172</td>
<td align="right">26</td>
<td>4</td>
</tr>
<tr>
<td align="right">92</td>
<td align="right">57</td>
<td align="right">196</td>
<td align="right">28</td>
<td>3</td>
</tr>
<tr>
<td align="right">93</td>
<td align="right">52</td>
<td align="right">200</td>
<td align="right">15</td>
<td>5</td>
</tr>
<tr>
<td align="right">94</td>
<td align="right">64</td>
<td align="right">162</td>
<td align="right">18</td>
<td>3</td>
</tr>
<tr>
<td align="right">95</td>
<td align="right">57</td>
<td align="right">168</td>
<td align="right">16</td>
<td>1</td>
</tr>
<tr>
<td align="right">96</td>
<td align="right">88</td>
<td align="right">171</td>
<td align="right">18</td>
<td>4</td>
</tr>
<tr>
<td align="right">97</td>
<td align="right">58</td>
<td align="right">160</td>
<td align="right">23</td>
<td>1</td>
</tr>
<tr>
<td align="right">98</td>
<td align="right">99</td>
<td align="right">155</td>
<td align="right">29</td>
<td>5</td>
</tr>
<tr>
<td align="right">99</td>
<td align="right">57</td>
<td align="right">168</td>
<td align="right">18</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>mendescribe kan kolom :</p>
<pre><code>df=pd.read_csv('Book1.csv',usecols=[&quot;beratbadan&quot;,&quot;tinggibadan&quot;,&quot;umur&quot;,&quot;banyaksaudara&quot;])
df.describe()
</code></pre>

<table>
<thead>
<tr>
<th align="right"></th>
<th align="right">beratbadan</th>
<th align="right">tinggibadan</th>
<th align="right">umur</th>
<th>banyaksaudara</th>
</tr>
</thead>
<tbody>
<tr>
<td align="right">count</td>
<td align="right">100.000000</td>
<td align="right">100.000000</td>
<td align="right">100.000000</td>
<td>100.000000</td>
</tr>
<tr>
<td align="right">mean</td>
<td align="right">73.190000</td>
<td align="right">175.070000</td>
<td align="right">22.390000</td>
<td>3.040000</td>
</tr>
<tr>
<td align="right">std</td>
<td align="right">13.813838</td>
<td align="right">14.286573</td>
<td align="right">4.483178</td>
<td>1.434918</td>
</tr>
<tr>
<td align="right">min</td>
<td align="right">50.000000</td>
<td align="right">150.000000</td>
<td align="right">15.000000</td>
<td>1.000000</td>
</tr>
<tr>
<td align="right">25%</td>
<td align="right">61.750000</td>
<td align="right">163.750000</td>
<td align="right">18.750000</td>
<td>2.000000</td>
</tr>
<tr>
<td align="right">50%</td>
<td align="right">71.000000</td>
<td align="right">174.000000</td>
<td align="right">22.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td align="right">75%</td>
<td align="right">86.000000</td>
<td align="right">186.250000</td>
<td align="right">26.250000</td>
<td>4.000000</td>
</tr>
<tr>
<td align="right">max</td>
<td align="right">99.000000</td>
<td align="right">200.000000</td>
<td align="right">30.000000</td>
<td>5.000000</td>
</tr>
</tbody>
</table>
<h5 id="ukuran-statistik-deskriptif">Ukuran statistik deskriptif</h5>
<p>Secara umum, ada 2 jenis pengukuran statistik deskriptif.</p>
<h6 id="1-ukuran-pemusatan">1. Ukuran pemusatan</h6>
<p>Ukuran pemusatan adalah metode paling lazim yang digunakan dalam analisis deskriptif. Metode ini fokus untuk menggambarkan kondisi data di titik pusat. Secara umum, kita bisa melihat bagaimana kondisi data dengan melihat dimana letak pusat data tersebut. Biasanya, pusat data sendiri akan berada pada nilai tengah, meskipun tida selalu demikian. Untuk membuktikan hal ini secara matematis maka pengukuran yang sering digunakan adalah mean, median, dan modus. Kita bahas satu per satu.</p>
<p>$$</p>
<p>$$</p>
<ol>
<li><strong>Mean</strong> merupakan rata-rata dari sekumpulan data yang kita miliki. Formulanya sangat sederhana. Anda hanya perlu menjumlah nilai dari seluruh data yang dimiliki dan membaginya dengan jumlah data tersebut.</li>
</ol>
<pre><code>print(&quot;rata-rata   &quot;,df['beratbadan'].mean())
</code></pre>

<ol>
<li><strong>Median</strong> adalah nilai tengah dari sebuah data. Bila kita memiliki sekumpulan data, kita bisa mengurutkan data tersebut dari nilai terkecil hingga terbesar. Jika kita memiliki jumlah data ganjil, maka nilai tengah data tersebut akan langsung menjadi median. Namun bila kita memiliki data genap, kita perlu menemukan nilai rata-rata dari nilai tengah data tersebut.</li>
</ol>
<pre><code>print(&quot;median          &quot;,df['beratbadan'].quantile(0.5))
</code></pre>

<ol>
<li><strong>Modus</strong> adalah nilai yang paling sering muncul dalam sekelompok data. Kita hanya perlu melihat nilai mana yang paling sering muncul dalam kelompok tersebut. Bila jumlah frekuensi setiap data sama, maka nilai modus tidak ada.</li>
</ol>
<pre><code>mode=stats.mode(df)
print(&quot;Nilai modus {} dengan jumlah {}&quot;.format(mode.mode[0], mode.count[0]))
</code></pre>

<h6 id="2-ukuran-keragaman">2. Ukuran keragaman</h6>
<p>Ukuran keragaman merupakan ukuran untuk menyajikan bagaimana sebaran dari data tersebut. Ukuran keragaman menunjukkan bagaimana kondisi sebuah data menyebar di kelompok data yang kita miliki. Hal ini memungkinkan kita untuk menganalisis seberapa jauh data-data tersebut tersebar dari ukuran pemusatannya. Bila sebaran datanya rendah, ini menunjukkan bahwa data tersebar tidak jauh dari pusatnya. Bila sebarannya jauh ini menunjukkan bahwa data tersebar jauh dari pusatnya.</p>
<ol>
<li><strong>Range</strong></li>
</ol>
<p>Range atau rentang merupakan selisih dari nilai terbesar dan nilai terkecil yang kita miliki. Range merupkan hal yang paling sederhana dan paling mudah dimengerti dalam ukuran penyebaran. Range menunjukkan seberapa jauh sebaran dengan mengabaikan bentuk distribusinya.</p>
<ol>
<li><strong>Quartiles Range</strong></li>
</ol>
<p>Rentang Quartiles atau rentang kuartil merupakan ukuran penyebaran yang membagi data menjadi 4 bagian. Sesuai dengan namanya, kuartil membagi data menjadi 25 persen di setiap bagiannya.</p>
<p>Ada 3 jenis nilai kuartil yang perlu kita tahu :</p>
<ul>
<li>Q1 atau kuartil bawah yang memuat 25 persen dari data dengan nilai terendah</li>
<li>Q2 atau kuartil tengah, yang membagi data menjadi 2 bagian sama besar 50 persen terkecil dan 50 persen terbesar. Q2 juga memiliki nilai yang sama dengan median</li>
<li>Q3 atau kuartil atas yang memuat 25 persen dari data dengan nilai tertinggi.</li>
</ul>
<pre><code>print(&quot;Q1       &quot;,df['beratbadan'].quantile(0.25))
print(&quot;Q2       &quot;,df['beratbadan'].quantile(0.5))
print(&quot;Q3       &quot;,df['beratbadan'].quantile(0.75))
</code></pre>

<ol>
<li><strong>Persentil</strong></li>
</ol>
<p>Persentil merupakan ukuran penyebaran yang membagi data menjadi 100 bagian sama besar.</p>
<ol>
<li><strong>Desil</strong></li>
</ol>
<p>Desil merupakan ukuran penyebaran yang membagi data menjadi 10 bagian sama besar.</p>
<ol>
<li><strong>Varians</strong></li>
</ol>
<p>Varians merupakan ukuran seberapa jauh menyebar dari nilai rata-ratanya. Semakin kecil nilai varians, semakin dekat sebaran data dengan rata-rata. Semakin besar nilai varian, semakin besar sebaran data terhadap nilai rata-ratanya.</p>
<pre><code>print(&quot;Variansi         &quot;,&quot;{0:.2f}&quot;.format(round(df['beratbadan'].var(),2)))
</code></pre>

<ol>
<li><strong>Standar deviasi</strong></li>
</ol>
<p>Standar deviasi merupakan ukuran lain dari sebaran data terhadap rata-ratanya. Bila anda menggunakan varians, maka nilai yang anda dapatkan sangatlah besar. Nilai ini tidak mampu menggambarkan bagaimana sebaran data yang sebenarnya terhadap rata-rata. Untuk mendapatkan nilai yang lebih mudah diinterpretasikan, standar deviasi adalah ukuran yang lebih tepat. Standar deviasi menghasilkan nilai yang lebih kecil dan mampu menjelaskan bagaimana sebaran data terhadap rata-rata. Standar deviasi disebut juga dengan simpangan baku.</p>
<pre><code>print(&quot;Standar Deviasi   &quot;,&quot;{0:.2f}&quot;.format(round(df['beratbadan'].std(),2)))
</code></pre>

<ol>
<li><strong>Skewness</strong></li>
</ol>
<p>Skewness merupakan ukuran yang menunjukkan bagaimana kemencengan sebuah data terhadap rata-ratanya. Skewness juga bisa dikatakan sebagai ukuran ketidaksimetrisan sebuah data.</p>
<ul>
<li>Sk &gt; 0 artinya kurva dikatakan menceng kanan (positif)</li>
<li>Sk = 0 artinya kurva normal </li>
<li>Sk &lt; 0 artinya menceng kiri (negatif)</li>
</ul>
<pre><code>print(&quot;kemencengan          &quot; ,&quot;{0:.6f}&quot;.format(round(df['beratbadan'].skew(),6)))
</code></pre>

<h5 id="code">Code</h5>
<pre><code>import pandas as pd
from scipy import stats
df=pd.read_csv(&quot;Book1.csv&quot;,usecols=[0])

print(&quot;jumlah data  &quot;,df['beratbadan'].count())
print(&quot;rata-rata   &quot;,df['beratbadan'].mean())
print(&quot;nila minimal &quot;,df['beratbadan'].min())
print(&quot;Q1       &quot;,df['beratbadan'].quantile(0.25))
print(&quot;Q2          &quot;,df['beratbadan'].quantile(0.5))
print(&quot;Q3          &quot;,df['beratbadan'].quantile(0.75))
print(&quot;Nilai Max   &quot;,df['beratbadan'].max())
mode=stats.mode(df)
print(&quot;Nilai modus {} dengan jumlah {}&quot;.format(mode.mode[0], mode.count[0]))
print(&quot;kemencengan          &quot; ,&quot;{0:.6f}&quot;.format(round(df['beratbadan'].skew(),6)))
print(&quot;Standar Deviasi   &quot;,&quot;{0:.2f}&quot;.format(round(df['beratbadan'].std(),2)))
print(&quot;Variansi         &quot;,&quot;{0:.2f}&quot;.format(round(df['beratbadan'].var(),2)))
</code></pre>

<p>hasil :</p>
<pre><code>jumlah data   100
rata-rata    73.19
nila minimal  50
Q1        61.75
Q2           71.0
Q3           86.0
Nilai Max    99
kemencengan 0.19
Nilai modus [72] dengan jumlah [5]
kemencengan           0.185820
Standar Deviasi    13.81
Variansi          190.82
</code></pre>

<hr>

<p><hr>
<hr>~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</hr></p>
<h3 id="tugas-20"><u><strong>TUGAS 2.0</strong></u></h3>
<h5 id="mengukur-jarak-data"><strong>Mengukur Jarak Data</strong></h5>
<h5 id="mengukur-jarak-tipe-numerik"><strong>Mengukur Jarak Tipe Numerik</strong></h5>
<p>Salah satu tantangan dalam era ini dengan datatabase yang memiliki banyak tipe data. Mengukur jarak adalah komponen utama dalam algoritma clustering berbasis jarak. Alogritma seperit Algoritma Partisioning misal K-Mean, K-medoidm dan fuzzy c-mean dan rough clustering bergantung pada jarak untuk melakukan pengelompokkan</p>
<p>Sebelum menjelaskan tentang beberapa macam ukuran jarak, kita mendefinisikan terlebih dahulu yaiut v1,v2v1,v2 menyatakandua vektor yang menyatakan v1=x1,x2,...,xn,v2=y1,y2,...,yn,v1=x1,x2,...,xn,v2=y1,y2,...,yn, dimana xi,yixi,yi disebut attribut. Ada beberapa ukuran similaritas datau ukuran jarak, diantaranya:</p>
<ol>
<li><em>Minkowski Distance</em></li>
<li><em>Manhattan distance</em></li>
<li><em>Euclidean distance</em></li>
<li><em>Average Distance</em></li>
<li><em>Weighted euclidean distance</em></li>
<li><em>Chord distance</em></li>
<li><em>Mahalanobis distance</em></li>
<li><em>Cosine measure</em></li>
<li><em>Pearson correlation</em></li>
</ol>
<pre><code>from scipy import stats
import pandas as pd
df = pd.read_csv('bupa.csv')
a=df.iloc[33:38]
a
</code></pre>

<p><strong>out:</strong></p>
<table>
<thead>
<tr>
<th align="right"></th>
<th align="right">a</th>
<th align="right">b</th>
<th align="right">c</th>
<th align="right">d</th>
<th align="right">e</th>
<th align="right">f</th>
</tr>
</thead>
<tbody>
<tr>
<td align="right">33</td>
<td align="right">88</td>
<td align="right">96</td>
<td align="right">28</td>
<td align="right">21</td>
<td align="right">3</td>
<td align="right">1</td>
</tr>
<tr>
<td align="right">34</td>
<td align="right">94</td>
<td align="right">65</td>
<td align="right">22</td>
<td align="right">18</td>
<td align="right">5</td>
<td align="right">1</td>
</tr>
<tr>
<td align="right">35</td>
<td align="right">91</td>
<td align="right">72</td>
<td align="right">155</td>
<td align="right">68</td>
<td align="right">3</td>
<td align="right">2</td>
</tr>
<tr>
<td align="right">36</td>
<td align="right">85</td>
<td align="right">54</td>
<td align="right">47</td>
<td align="right">33</td>
<td align="right">3</td>
<td align="right">2</td>
</tr>
<tr>
<td align="right">37</td>
<td align="right">79</td>
<td align="right">39</td>
<td align="right">14</td>
<td align="right">19</td>
<td align="right">3</td>
<td align="right">2</td>
</tr>
</tbody>
</table>
<p><strong>kolom kategori :</strong></p>
<pre><code>binary=[5]
ordinal=[4]
num=[0,1,2,3]
</code></pre>

<p><strong>Menghitung Jarak Numerik</strong> (averagedistance)</p>
<pre><code>def chordDist(v1,v2,jnis):
    jmlh=0
    normv1=0
    normv2=0
    for x in range (len(jnis)):
        normv1=normv1+(int(a.values.tolist()[v1][jnis[x]])**2)
        normv2=normv2+(int(a.values.tolist()[v2][jnis[x]])**2)
        jmlh=jmlh+(int(a.values.tolist()[v1][jnis[x]])*int(a.values.tolist()[v2][jnis[x]]))
    return ((2-(2*jmlh/(normv1*normv2)))**0.5)    
</code></pre>

<h5 id="menghitung-jarak-ordinal"><strong>Menghitung Jarak Ordinal</strong></h5>
<p>Nilai-nilai atribut ordinal memiliki urutan atau peringkat, namun besarnya antara nilai-nilai berturut-turut tidak diketahui. Contohnya tingkatan kecil, sedang, besar untuk atribut ukuran. Atribut ordinal juga dapat diperoleh dari diskritisasi atribut numerik dengan membuat rentang nilai ke dalam sejumlah kategori tertentu. Kategori-kategori ini disusun dalam peringkat. Yaitu, rentang atribut numerik dapat dipetakan ke atribut ordinal ff yang memiliki MfMf state. Misalnya, kisaran suhu atribut skala-skala (dalam Celcius)dapat diatur ke dalam status berikut: −30 hingga −10, −10 hingga 10, 10 hingga 30, masing-masing mewakili kategori suhu dingin, suhu sedang, dan suhu hangat. MM adalah jumlah keadaan yang dapat dilakukan oleh atribut ordinalmemiliki. State ini menentukan peringkat 1,...,Mf</p>
<pre><code>def ordDist(v1,v2,jnis):
    jmlh=0
    for x in range (len(jnis)):
        z1=int(a.values.tolist()[v1][jnis[x]])-1
        z2=int(a.values.tolist()[v2][jnis[x]])-1
        jmlh=jmlh+chordDist(z1,z2,jnis)
    return (jmlh)
</code></pre>

<h5 id="menghitung-jarak-binary"><strong>Menghitung jarak Binary</strong></h5>
<p>Mari kita lihat similaritas dan desimilirity untuk objek yang dijelaskan oleh atribut biner simetris atau asimetris. Aatribut biner hanya memiliki dua status: 0 dan 1 Contoh atribut perokok menggambarkan seorang pasien, misalnya, 1 menunjukkan bahwa pasien merokok, sedangkan 0 menunjukkan pasien tidak merokok. Memperlakukan atribut biner sebagai atribut numerik tidak diperkenankan. Oleh karena itu, metode khusus untuk data biner diperlukan untuk membedakan komputasi.</p>
<p>Jadi, bagaimana kita bisa menghitung ketidaksamaan antara dua atribut biner? ”Satu pendekatan melibatkan penghitungan matriks ketidaksamaan dari data biner yang diberikan. Jika semua atribut biner dianggap memiliki bobot yang sama, kita memiliki tabel kontingensi 2×22×2 di mana qq adalah jumlah atribut yang sama dengan 1 untuk kedua objek ii dan jj, rr adalah jumlah atribut yang sama dengan 1 untuk objek ii tetapi 0 untuk objek jj, ss adalah jumlah atribut yang sama dengan 0 untuk objek ii tetapi 1 untuk objek jj, dan tt adalah jumlah atribut yang sama dengan 0 untuk kedua objek ii dan jj. Jumlah total atribut adalah pp, di mana p=q+r+s+t</p>
<pre><code>def binaryDist(v1,v2,jnis):
    q=0
    r=0
    s=0
    t=0
    for x in range (len(jnis)):
        if (int(a.values.tolist()[v1][jnis[x]]))==1 and (int(a.values.tolist()[v2][jnis[x]]))==1:
            q=q+1
        elif (int(a.values.tolist()[v1][jnis[x]]))==1 and (int(a.values.tolist()[v2][jnis[x]]))==2:
            r=r+1
        elif (int(a.values.tolist()[v1][jnis[x]]))==2 and (int(a.values.tolist()[v2][jnis[x]]))==1:
            s=s+1
        else:
            t=t+1
    return ((r+s)/(q+r+s+t))
</code></pre>

<h5 id="menghitung-jarak-campuran">Menghitung Jarak Campuran</h5>
<p>jumlah jarak setiap jenis type data</p>
<pre><code>def jarak(v1,v2):
    return ((chordDist(v1,v2,num)+ordDist(v1,v2,ordinal)+binaryDist(v1,v2,binary))/4)
</code></pre>

<h5 id="display"><strong>display </strong></h5>
<pre><code>from IPython.display import HTML, display
import tabulate
table=[
    [&quot;Data&quot;]+[&quot;Jarak&quot;]+[&quot;Numeric&quot;]+[&quot;Ordinal&quot;]+[&quot;Binary&quot;],
    [&quot;v1-v2&quot;]+[&quot;{:.3f}&quot;.format(jarak(0,1))]+[&quot;{:.3f}&quot;.format(chordDist(0,1,num))]+[&quot;{:.3f}&quot;.format(ordDist(0,1,ordinal))]+[&quot;{:.3f}&quot;.format(binaryDist(0,1,binary))],
    [&quot;v1-v3&quot;]+[&quot;{:.3f}&quot;.format(jarak(0,2))]+[&quot;{:.3f}&quot;.format(chordDist(0,3,num))]+[&quot;{:.3f}&quot;.format(ordDist(0,2,ordinal))]+[&quot;{:.3f}&quot;.format(binaryDist(0,2,binary))],
    [&quot;v2-v3&quot;]+[&quot;{:.3f}&quot;.format(jarak(1,2))]+[&quot;{:.3f}&quot;.format(chordDist(1,2,num))]+[&quot;{:.3f}&quot;.format(ordDist(1,2,ordinal))]+[&quot;{:.3f}&quot;.format(binaryDist(1,2,binary))],
    [&quot;v2-v4&quot;]+[&quot;{:.3f}&quot;.format(jarak(1,3))]+[&quot;{:.3f}&quot;.format(chordDist(1,3,num))]+[&quot;{:.3f}&quot;.format(ordDist(1,3,ordinal))]+[&quot;{:.3f}&quot;.format(binaryDist(1,3,binary))],
    ]

display(HTML(tabulate.tabulate(table, tablefmt='html')))
</code></pre>

<p><strong>out :</strong></p>
<table>
<thead>
<tr>
<th>Data</th>
<th>Jarak</th>
<th>Numeric</th>
<th>Ordinal</th>
<th>Binary</th>
</tr>
</thead>
<tbody>
<tr>
<td>v1-v2</td>
<td>0.687</td>
<td>1.414</td>
<td>1.333</td>
<td>0.000</td>
</tr>
<tr>
<td>v1-v3</td>
<td>0.937</td>
<td>1.414</td>
<td>1.333</td>
<td>1.000</td>
</tr>
<tr>
<td>v2-v3</td>
<td>0.937</td>
<td>1.414</td>
<td>1.333</td>
<td>1.000</td>
</tr>
<tr>
<td>v2-v4</td>
<td>0.937</td>
<td>1.414</td>
<td>1.333</td>
<td>1.000</td>
</tr>
</tbody>
</table>
<p>source : - task2.ipynb &amp; bupa.csv</p>
<p><a href="https://github.com/erfiandanuri80/Asli_PENAMBANGAN_DATA/tree/master/asset">https://github.com/erfiandanuri80/Asli_PENAMBANGAN_DATA/tree/master/asset</a></p>
<hr>

<h3 id="tugas-30"><u><strong>TUGAS 3.0</strong></u></h3>
<h5 id="seleksi-fitur"><strong>Seleksi Fitur</strong></h5>
<p>Seleksi fitur adalah teknik untuk memilih fitur penting dan relevan terhadap data dan mengurangi fitur yang tidak relevan. Seleksi fitur bertujuan untuk memilih fitur terbaik dari suatu kumpulan data fitur. Seleksi fitur bertujuan untuk memilih fitur terbaik dari suatu kumpulan data fitur. Tujuan dari penelitian ini adalah menerapkan metode Information Gain dalam sistem seleksi fitur untuk Permasalahan cuaca . Metode Information Gain adalah metode yang menggunakan teknik scoring untuk pembobotan sebuah fitur dengan menggunakan maksimal entropy. Fitur yang dipilih adalah fitur dengan nilai Information Gain yang lebih besar atau sama dengan nilai threshold tertentu.</p>
<pre><code>from pandas import *
from IPython.display import HTML, display
from tabulate import tabulate
from math import log
from sklearn.feature_selection import mutual_info_classif


def table(df): 
    display(HTML(tabulate(df, tablefmt='html', headers='keys', showindex=False)))
</code></pre>

<pre><code>df = read_csv('feature_selection.csv', sep=';')
table(df)
</code></pre>

<table>
<thead>
<tr>
<th align="center">outlook</th>
<th align="center">temperature</th>
<th align="center">humidity</th>
<th align="left">windy</th>
<th align="center">play</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">sunny</td>
<td align="center">hot</td>
<td align="center">high</td>
<td align="left">False</td>
<td align="center">no</td>
</tr>
<tr>
<td align="center">sunny</td>
<td align="center">hot</td>
<td align="center">high</td>
<td align="left">True</td>
<td align="center">no</td>
</tr>
<tr>
<td align="center">overcast</td>
<td align="center">hot</td>
<td align="center">high</td>
<td align="left">False</td>
<td align="center">yes</td>
</tr>
<tr>
<td align="center">rainy</td>
<td align="center">mild</td>
<td align="center">high</td>
<td align="left">False</td>
<td align="center">yes</td>
</tr>
<tr>
<td align="center">rainy</td>
<td align="center">cool</td>
<td align="center">normal</td>
<td align="left">False</td>
<td align="center">yes</td>
</tr>
<tr>
<td align="center">rainy</td>
<td align="center">cool</td>
<td align="center">normal</td>
<td align="left">True</td>
<td align="center">no</td>
</tr>
<tr>
<td align="center">overcast</td>
<td align="center">cool</td>
<td align="center">normal</td>
<td align="left">True</td>
<td align="center">yes</td>
</tr>
<tr>
<td align="center">sunny</td>
<td align="center">mild</td>
<td align="center">high</td>
<td align="left">False</td>
<td align="center">no</td>
</tr>
<tr>
<td align="center">sunny</td>
<td align="center">cool</td>
<td align="center">normal</td>
<td align="left">False</td>
<td align="center">yes</td>
</tr>
<tr>
<td align="center">rainy</td>
<td align="center">mild</td>
<td align="center">normal</td>
<td align="left">False</td>
<td align="center">yes</td>
</tr>
<tr>
<td align="center">sunny</td>
<td align="center">mild</td>
<td align="center">normal</td>
<td align="left">True</td>
<td align="center">yes</td>
</tr>
<tr>
<td align="center">overcast</td>
<td align="center">mild</td>
<td align="center">high</td>
<td align="left">True</td>
<td align="center">yes</td>
</tr>
<tr>
<td align="center">overcast</td>
<td align="center">hot</td>
<td align="center">normal</td>
<td align="left">False</td>
<td align="center">yes</td>
</tr>
<tr>
<td align="center">rainy</td>
<td align="center">mild</td>
<td align="center">high</td>
<td align="left">True</td>
<td align="center">no</td>
</tr>
</tbody>
</table>
<h5 id="mencari-entropy">Mencari Entropy</h5>
<p>Untuk menghitung Information gain perlu dihitung dahulu nilai informasi dalam suatu bits dari suatu kumpulan obyek. Cara penghitungan dilakukan dengan menggunakan konsep entropi. Entropi menyatakan impurity suatu kumpulan obyek . Berikut merupakan definisi dari entropi suatu ruang sampel data (S):</p>
<p>$$
E(T) = \sum_{i=1}^n {-P_i\log{P_i}}
$$</p>
<pre><code>def findEntropy(column):
    rawGroups = df.groupby(column)
    targetGroups = [[key, len(data), len(data)/df[column].size] for key,data in rawGroups]
    targetGroups = DataFrame(targetGroups, columns=['value', 'count', 'probability'])
    return sum([-x*log(x,2) for x in targetGroups['probability']]), targetGroups, rawGroups

entropyTarget, groupTargets, _ = findEntropy('play')
table(groupTargets)
print('entropy target =', entropyTarget)
</code></pre>

<table>
<thead>
<tr>
<th align="center">value</th>
<th align="center">count</th>
<th align="center">probability</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">no</td>
<td align="center">5</td>
<td align="center">0.357143</td>
</tr>
<tr>
<td align="center">yes</td>
<td align="center">9</td>
<td align="center">0.642857</td>
</tr>
</tbody>
</table>
<pre><code>entropy target = 0.9402859586706309
</code></pre>

<h5 id="gain">Gain</h5>
<p>Gain adalah sebuah fiktur yang terdapat pada sebuah data , untuk menghitungnya contoh rumusnya :
$$
\operatorname{Gain}(T, X) = \operatorname{Entropy}(T) - \sum_{v\in{T}} \frac{T_{X,v}}{T} E(T_{X,v})
$$</p>
<pre><code>def findGain(column):
    entropyOutlook, groupOutlooks, rawOutlooks = findEntropy(column)
    table(groupOutlooks)
    gain = entropyTarget-sum(len(data)/len(df)*sum(-x/len(data)*log(x/len(data),2) 
                for x in data.groupby('play').size()) for key,data in rawOutlooks)
    print(&quot;gain of&quot;,column,&quot;is&quot;,gain)
    return gain
</code></pre>

<table>
<thead>
<tr>
<th align="center">value</th>
<th align="center">count</th>
<th align="center">probability</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">overcast</td>
<td align="center">4</td>
<td align="center">0.285714</td>
</tr>
<tr>
<td align="center">rainy</td>
<td align="center">5</td>
<td align="center">0.357143</td>
</tr>
<tr>
<td align="center">sunny</td>
<td align="center">5</td>
<td align="center">0.357143</td>
</tr>
</tbody>
</table>
<pre><code>gain of outlook is 0.2467498197744391
</code></pre>

<table>
<thead>
<tr>
<th align="center">value</th>
<th align="center">count</th>
<th align="center">probability</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">cool</td>
<td align="center">4</td>
<td align="center">0.285714</td>
</tr>
<tr>
<td align="center">hot</td>
<td align="center">4</td>
<td align="center">0.285714</td>
</tr>
<tr>
<td align="center">mild</td>
<td align="center">6</td>
<td align="center">0.428571</td>
</tr>
</tbody>
</table>
<pre><code>gain of temperature is 0.029222565658954647
</code></pre>

<table>
<thead>
<tr>
<th align="center">value</th>
<th align="center">count</th>
<th align="center">probability</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">high</td>
<td align="center">7</td>
<td align="center">0.5</td>
</tr>
<tr>
<td align="center">normal</td>
<td align="center">7</td>
<td align="center">0.5</td>
</tr>
</tbody>
</table>
<pre><code>gain of humidity is 0.15183550136234136
</code></pre>

<table>
<thead>
<tr>
<th align="center">value</th>
<th align="center">count</th>
<th align="center">probability</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">False</td>
<td align="center">8</td>
<td align="center">0.571429</td>
</tr>
<tr>
<td align="center">True</td>
<td align="center">6</td>
<td align="center">0.428571</td>
</tr>
</tbody>
</table>
<pre><code>gain of windy is 0.04812703040826927
</code></pre>

<h5 id="skor-keseluruhan-gain">Skor Keseluruhan Gain</h5>
<pre><code>table(DataFrame(gains, columns=[&quot;Feature&quot;, &quot;Gain Score&quot;]).sort_values(&quot;Gain Score&quot;)[::-1])
</code></pre>

<table>
<thead>
<tr>
<th align="center">Feature</th>
<th align="center">Gain Score</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">outlook</td>
<td align="center">0.24675</td>
</tr>
<tr>
<td align="center">humidity</td>
<td align="center">0.151836</td>
</tr>
<tr>
<td align="center">windy</td>
<td align="center">0.048127</td>
</tr>
<tr>
<td align="center">temperature</td>
<td align="center">0.0292226</td>
</tr>
</tbody>
</table>
<h3 id="tugas-40"><u>TUGAS 4.0</u></h3>
<p><img alt="img" src="https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1543836884/image_1_qegish.png" /></p>
<h5 id="naive-bayes-classifiers">Naive Bayes Classifiers</h5>
<p>Pengklasifikasi Naif Bayes adalah kumpulan algoritma klasifikasi berdasarkan <strong>Teorema Bayes</strong> . Ini bukan algoritma tunggal tetapi keluarga algoritma di mana mereka semua berbagi prinsip yang sama, yaitu setiap pasangan fitur yang diklasifikasi tidak tergantung satu sama lain. </p>
<h6 id="alur-kerja-klasifikasi">Alur Kerja Klasifikasi</h6>
<p>Setiap kali Anda melakukan klasifikasi, langkah pertama adalah memahami masalah dan mengidentifikasi fitur dan label potensial. Fitur adalah karakteristik atau atribut yang memengaruhi hasil label. Misalnya, dalam hal penyaluran pinjaman, manajer bank mengidentifikasi pekerjaan, pendapatan, usia, lokasi nasabah, riwayat pinjaman sebelumnya, riwayat transaksi, dan skor kredit. Karakteristik ini dikenal sebagai fitur yang membantu model mengklasifikasikan pelanggan.</p>
<p>Klasifikasi memiliki dua fase, fase pembelajaran, dan fase evaluasi. Pada fase pembelajaran, classifier melatih modelnya pada dataset yang diberikan dan pada fase evaluasi, ia menguji kinerja classifier. Kinerja dievaluasi berdasarkan berbagai parameter seperti akurasi, kesalahan, presisi, dan penarikan.</p>
<p><img alt="img" src="https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1543836883/image_2_rrxvol.png" /></p>
<p>Naive Bayes adalah teknik klasifikasi statistik berdasarkan Bayes Theorem. Ini adalah salah satu algoritma pembelajaran terawasi yang paling sederhana. Klasifikasi Naive Bayes adalah algoritma yang cepat, akurat dan andal. Klasifikasi Naif Bayes memiliki akurasi dan kecepatan tinggi pada dataset besar.</p>
<p>Klasifikasi Naive Bayes mengasumsikan bahwa efek fitur tertentu di kelas tidak tergantung pada fitur lainnya. Misalnya, pemohon pinjaman diinginkan atau tidak tergantung pada pendapatannya, pinjaman sebelumnya dan riwayat transaksi, usia, dan lokasi. Sekalipun fitur-fitur ini saling bergantung, fitur-fitur ini masih dianggap independen. Asumsi ini menyederhanakan perhitungan, dan itu sebabnya dianggap naif. Asumsi ini disebut independensi kondisional kelas.</p>
<h6 id="teorema-bayes"><strong>Teorema Bayes</strong></h6>
<p>Teorema Bayes menemukan probabilitas suatu peristiwa terjadi mengingat probabilitas peristiwa lain yang telah terjadi. Teorema Bayes dinyatakan secara matematis sebagai persamaan berikut:</p>
<p><img alt="img" src="https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1543836882/image_3_ijznzs.png" /></p>
<ul>
<li>P (h): probabilitas hipotesis h menjadi benar (terlepas dari data). Ini dikenal sebagai probabilitas sebelumnya dari h.</li>
<li>P (D): probabilitas data (terlepas dari hipotesis). Ini dikenal sebagai probabilitas sebelumnya.</li>
<li>P (h | D): probabilitas hipotesis h diberikan data D. Ini dikenal sebagai probabilitas posterior.</li>
<li>P (D | h): probabilitas data d mengingat bahwa hipotesis h adalah benar. Ini dikenal sebagai probabilitas posterior.</li>
</ul>
<p>Sekarang, sehubungan dengan dataset kami, kami dapat menerapkan teorema Bayes dengan cara berikut:</p>
<p><img src="https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-e85875a7ff9e9b557eab6281cc7ff078_l3.svg" alt=" P (y | X) = \ frac {P (X | y) P (y)} {P (X)} " style="zoom:150%;" /></p>
<p>di mana, y adalah variabel kelas dan X adalah vektor fitur dependen (ukuran <em>n</em> ) di mana:</p>
<p><img src="https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-5385a4693c3fb17811cf36593978a601_l3.svg" alt=" X = (x_1, x_2, x_3, ....., x_n) " style="zoom:150%;" /></p>
<p>Hanya untuk menghapus, contoh vektor fitur dan variabel kelas yang sesuai dapat berupa: (rujuk baris pertama dataset)</p>
<h6 id="pendekatan-pertama-dalam-hal-fitur-tunggal">Pendekatan Pertama (Dalam hal fitur tunggal)</h6>
<p>Pengklasifikasi Naive Bayes menghitung probabilitas suatu peristiwa dalam langkah-langkah berikut:</p>
<ul>
<li>Langkah 1: Hitung probabilitas sebelumnya untuk label kelas yang diberikan</li>
<li>Langkah 2: Temukan probabilitas Peluang dengan setiap atribut untuk setiap kelas</li>
<li>Langkah 3: Masukkan nilai ini dalam Formula Bayes dan hitung probabilitas posterior.</li>
<li>Langkah 4: Lihat kelas mana yang memiliki probabilitas lebih tinggi, mengingat input milik kelas probabilitas lebih tinggi.</li>
</ul>
<p>Untuk menyederhanakan perhitungan probabilitas sebelum dan posterior Anda dapat menggunakan tabel dua frekuensi dan kemungkinan. Kedua tabel ini akan membantu Anda menghitung probabilitas sebelum dan belakang. Tabel Frekuensi berisi kemunculan label untuk semua fitur. Ada dua tabel kemungkinan. Kemungkinan Tabel 1 menunjukkan probabilitas label sebelumnya dan Kemungkinan Tabel 2 menunjukkan probabilitas posterior.</p>
<p><img alt="img" src="https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1543836883/image_4_lyi0ob.png" /></p>
<h6 id="pendekatan-kedua-dalam-hal-banyak-fitur">Pendekatan Kedua (Dalam hal banyak fitur)</h6>
<p><img alt="img" src="https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1543836884/image_5_uhsgzr.png" /></p>
<h5 id="bangunan-classifier-di-scikit-learn">Bangunan Classifier di Scikit-learn</h5>
<h6 id="klasifikasi-naif-bayes"><strong>Klasifikasi Naif Bayes</strong></h6>
<h6 id="mendefinisikan-dataset"><strong>Mendefinisikan Dataset</strong></h6>
<p><strong>Naif Bayes dengan Banyak Label</strong></p>
<p>Sampai sekarang Anda telah belajar klasifikasi Naif Bayes dengan label biner. Sekarang Anda akan belajar tentang klasifikasi beberapa kelas di Naif Bayes. Yang dikenal sebagai klasifikasi multinomial Naive Bayes. Misalnya, jika Anda ingin mengklasifikasikan artikel berita tentang teknologi, hiburan, politik, atau olahraga.</p>
<p>Pada bagian pembuatan model, Anda dapat menggunakan dataset iris yang merupakan  <a href="https://en.wikipedia.org/wiki/Multivariate_statistics">multivariat </a><a href="https://en.wikipedia.org/wiki/Data_set">kumpulan data</a> diperkenalkan oleh British <a href="https://en.wikipedia.org/wiki/Statistician">statistik</a> dan <a href="https://en.wikipedia.org/wiki/Biologist">biologi </a><a href="https://en.wikipedia.org/wiki/Ronald_Fisher">Ronald Fisher</a> pada tahun 1936 makalahnya <em>Penggunaan beberapa pengukuran di masalah taksonomi</em> sebagai contoh <a href="https://en.wikipedia.org/wiki/Linear_discriminant_analysis">analisis diskriminan linier</a> . Kadang-kadang disebut set data Iris Anderson karena <a href="https://en.wikipedia.org/wiki/Edgar_Anderson">Edgar Anderson</a> mengumpulkan data untuk menghitung variasi <a href="https://en.wikipedia.org/wiki/Morphology_(biology)">morfologis</a> bunga <em>Iris</em> dari tiga spesies terkait.Dua dari tiga spesies dikumpulkan di<a href="https://en.wikipedia.org/wiki/Gaspé_Peninsula"> Semenanjung Gaspé</a> "semuanya berasal dari padang rumput yang sama, dan dipetik pada hari yang sama dan diukur pada saat yang sama oleh orang yang sama dengan peralatan yang sama". </p>
<p>Dataset terdiri dari 50 sampel dari masing-masing dari tiga spesies <em>Iris</em> ( <em>Iris setosa</em> , <em>Iris virginica</em> dan <em>Iris versicolor</em> ). Empat <a href="https://en.wikipedia.org/wiki/Features_(pattern_recognition)">fitur</a> diukur dari masing-masing sampel: panjang dan lebar <a href="https://en.wikipedia.org/wiki/Sepal">sepal</a> dan <a href="https://en.wikipedia.org/wiki/Petal">kelopak</a> , dalam sentimeter. Berdasarkan kombinasi keempat fitur ini, Fisher mengembangkan model diskriminan linier untuk membedakan spesies dari satu sama lain. Dataset tersedia di perpustakaan scikit-learn.</p>
<pre><code>dataset = pd.read_csv('iris.csv',sep=&quot;,&quot;)
dataset
</code></pre>

<p>output :</p>
<table>
<thead>
<tr>
<th align="right">sepal_length</th>
<th align="right">sepal_width</th>
<th align="right">petal_length</th>
<th align="right">petal_width</th>
<th align="right">species</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td align="right">0</td>
<td align="right">5.1</td>
<td align="right">3.5</td>
<td align="right">1.4</td>
<td align="right">0.2</td>
<td>setosa</td>
</tr>
<tr>
<td align="right">1</td>
<td align="right">4.9</td>
<td align="right">3.0</td>
<td align="right">1.4</td>
<td align="right">0.2</td>
<td>setosa</td>
</tr>
<tr>
<td align="right">2</td>
<td align="right">4.7</td>
<td align="right">3.2</td>
<td align="right">1.3</td>
<td align="right">0.2</td>
<td>setosa</td>
</tr>
<tr>
<td align="right">3</td>
<td align="right">4.6</td>
<td align="right">3.1</td>
<td align="right">1.5</td>
<td align="right">0.2</td>
<td>setosa</td>
</tr>
<tr>
<td align="right">4</td>
<td align="right">5.0</td>
<td align="right">3.6</td>
<td align="right">1.4</td>
<td align="right">0.2</td>
<td>setosa</td>
</tr>
<tr>
<td align="right">5</td>
<td align="right">5.4</td>
<td align="right">3.9</td>
<td align="right">1.7</td>
<td align="right">0.4</td>
<td>setosa</td>
</tr>
<tr>
<td align="right">6</td>
<td align="right">4.6</td>
<td align="right">3.4</td>
<td align="right">1.4</td>
<td align="right">0.3</td>
<td>setosa</td>
</tr>
<tr>
<td align="right">7</td>
<td align="right">5.0</td>
<td align="right">3.4</td>
<td align="right">1.5</td>
<td align="right">0.2</td>
<td>setosa</td>
</tr>
<tr>
<td align="right">8</td>
<td align="right">4.4</td>
<td align="right">2.9</td>
<td align="right">1.4</td>
<td align="right">0.2</td>
<td>setosa</td>
</tr>
<tr>
<td align="right">9</td>
<td align="right">4.9</td>
<td align="right">3.1</td>
<td align="right">1.5</td>
<td align="right">0.1</td>
<td>setosa</td>
</tr>
<tr>
<td align="right">10</td>
<td align="right">5.4</td>
<td align="right">3.7</td>
<td align="right">1.5</td>
<td align="right">0.2</td>
<td>setosa</td>
</tr>
<tr>
<td align="right">11</td>
<td align="right">4.8</td>
<td align="right">3.4</td>
<td align="right">1.6</td>
<td align="right">0.2</td>
<td>setosa</td>
</tr>
<tr>
<td align="right">12</td>
<td align="right">4.8</td>
<td align="right">3.0</td>
<td align="right">1.4</td>
<td align="right">0.1</td>
<td>setosa</td>
</tr>
<tr>
<td align="right">13</td>
<td align="right">4.3</td>
<td align="right">3.0</td>
<td align="right">1.1</td>
<td align="right">0.1</td>
<td>setosa</td>
</tr>
<tr>
<td align="right">14</td>
<td align="right">5.8</td>
<td align="right">4.0</td>
<td align="right">1.2</td>
<td align="right">0.2</td>
<td>setosa</td>
</tr>
<tr>
<td align="right">15</td>
<td align="right">5.7</td>
<td align="right">4.4</td>
<td align="right">1.5</td>
<td align="right">0.4</td>
<td>setosa</td>
</tr>
<tr>
<td align="right">16</td>
<td align="right">5.4</td>
<td align="right">3.9</td>
<td align="right">1.3</td>
<td align="right">0.4</td>
<td>setosa</td>
</tr>
<tr>
<td align="right">17</td>
<td align="right">5.1</td>
<td align="right">3.5</td>
<td align="right">1.4</td>
<td align="right">0.3</td>
<td>setosa</td>
</tr>
<tr>
<td align="right">18</td>
<td align="right">5.7</td>
<td align="right">3.8</td>
<td align="right">1.7</td>
<td align="right">0.3</td>
<td>setosa</td>
</tr>
<tr>
<td align="right">19</td>
<td align="right">5.1</td>
<td align="right">3.8</td>
<td align="right">1.5</td>
<td align="right">0.3</td>
<td>setosa</td>
</tr>
<tr>
<td align="right">20</td>
<td align="right">5.4</td>
<td align="right">3.4</td>
<td align="right">1.7</td>
<td align="right">0.2</td>
<td>setosa</td>
</tr>
<tr>
<td align="right">21</td>
<td align="right">5.1</td>
<td align="right">3.7</td>
<td align="right">1.5</td>
<td align="right">0.4</td>
<td>setosa</td>
</tr>
<tr>
<td align="right">22</td>
<td align="right">4.6</td>
<td align="right">3.6</td>
<td align="right">1.0</td>
<td align="right">0.2</td>
<td>setosa</td>
</tr>
<tr>
<td align="right">23</td>
<td align="right">5.1</td>
<td align="right">3.3</td>
<td align="right">1.7</td>
<td align="right">0.5</td>
<td>setosa</td>
</tr>
<tr>
<td align="right">24</td>
<td align="right">4.8</td>
<td align="right">3.4</td>
<td align="right">1.9</td>
<td align="right">0.2</td>
<td>setosa</td>
</tr>
<tr>
<td align="right">25</td>
<td align="right">5.0</td>
<td align="right">3.0</td>
<td align="right">1.6</td>
<td align="right">0.2</td>
<td>setosa</td>
</tr>
<tr>
<td align="right">26</td>
<td align="right">5.0</td>
<td align="right">3.4</td>
<td align="right">1.6</td>
<td align="right">0.4</td>
<td>setosa</td>
</tr>
<tr>
<td align="right">27</td>
<td align="right">5.2</td>
<td align="right">3.5</td>
<td align="right">1.5</td>
<td align="right">0.2</td>
<td>setosa</td>
</tr>
<tr>
<td align="right">28</td>
<td align="right">5.2</td>
<td align="right">3.4</td>
<td align="right">1.4</td>
<td align="right">0.2</td>
<td>setosa</td>
</tr>
<tr>
<td align="right">29</td>
<td align="right">4.7</td>
<td align="right">3.2</td>
<td align="right">1.6</td>
<td align="right">0.2</td>
<td>setosa</td>
</tr>
<tr>
<td align="right">...</td>
<td align="right">...</td>
<td align="right">...</td>
<td align="right">...</td>
<td align="right">...</td>
<td>...</td>
</tr>
<tr>
<td align="right">120</td>
<td align="right">6.9</td>
<td align="right">3.2</td>
<td align="right">5.7</td>
<td align="right">2.3</td>
<td>virginica</td>
</tr>
<tr>
<td align="right">121</td>
<td align="right">5.6</td>
<td align="right">2.8</td>
<td align="right">4.9</td>
<td align="right">2.0</td>
<td>virginica</td>
</tr>
<tr>
<td align="right">122</td>
<td align="right">7.7</td>
<td align="right">2.8</td>
<td align="right">6.7</td>
<td align="right">2.0</td>
<td>virginica</td>
</tr>
<tr>
<td align="right">123</td>
<td align="right">6.3</td>
<td align="right">2.7</td>
<td align="right">4.9</td>
<td align="right">1.8</td>
<td>virginica</td>
</tr>
<tr>
<td align="right">124</td>
<td align="right">6.7</td>
<td align="right">3.3</td>
<td align="right">5.7</td>
<td align="right">2.1</td>
<td>virginica</td>
</tr>
<tr>
<td align="right">125</td>
<td align="right">7.2</td>
<td align="right">3.2</td>
<td align="right">6.0</td>
<td align="right">1.8</td>
<td>virginica</td>
</tr>
<tr>
<td align="right">126</td>
<td align="right">6.2</td>
<td align="right">2.8</td>
<td align="right">4.8</td>
<td align="right">1.8</td>
<td>virginica</td>
</tr>
<tr>
<td align="right">127</td>
<td align="right">6.1</td>
<td align="right">3.0</td>
<td align="right">4.9</td>
<td align="right">1.8</td>
<td>virginica</td>
</tr>
<tr>
<td align="right">128</td>
<td align="right">6.4</td>
<td align="right">2.8</td>
<td align="right">5.6</td>
<td align="right">2.1</td>
<td>virginica</td>
</tr>
<tr>
<td align="right">129</td>
<td align="right">7.2</td>
<td align="right">3.0</td>
<td align="right">5.8</td>
<td align="right">1.6</td>
<td>virginica</td>
</tr>
<tr>
<td align="right">130</td>
<td align="right">7.4</td>
<td align="right">2.8</td>
<td align="right">6.1</td>
<td align="right">1.9</td>
<td>virginica</td>
</tr>
<tr>
<td align="right">131</td>
<td align="right">7.9</td>
<td align="right">3.8</td>
<td align="right">6.4</td>
<td align="right">2.0</td>
<td>virginica</td>
</tr>
<tr>
<td align="right">132</td>
<td align="right">6.4</td>
<td align="right">2.8</td>
<td align="right">5.6</td>
<td align="right">2.2</td>
<td>virginica</td>
</tr>
<tr>
<td align="right">133</td>
<td align="right">6.3</td>
<td align="right">2.8</td>
<td align="right">5.1</td>
<td align="right">1.5</td>
<td>virginica</td>
</tr>
<tr>
<td align="right">134</td>
<td align="right">6.1</td>
<td align="right">2.6</td>
<td align="right">5.6</td>
<td align="right">1.4</td>
<td>virginica</td>
</tr>
<tr>
<td align="right">135</td>
<td align="right">7.7</td>
<td align="right">3.0</td>
<td align="right">6.1</td>
<td align="right">2.3</td>
<td>virginica</td>
</tr>
<tr>
<td align="right">136</td>
<td align="right">6.3</td>
<td align="right">3.4</td>
<td align="right">5.6</td>
<td align="right">2.4</td>
<td>virginica</td>
</tr>
<tr>
<td align="right">137</td>
<td align="right">6.4</td>
<td align="right">3.1</td>
<td align="right">5.5</td>
<td align="right">1.8</td>
<td>virginica</td>
</tr>
<tr>
<td align="right">138</td>
<td align="right">6.0</td>
<td align="right">3.0</td>
<td align="right">4.8</td>
<td align="right">1.8</td>
<td>virginica</td>
</tr>
<tr>
<td align="right">139</td>
<td align="right">6.9</td>
<td align="right">3.1</td>
<td align="right">5.4</td>
<td align="right">2.1</td>
<td>virginica</td>
</tr>
<tr>
<td align="right">140</td>
<td align="right">6.7</td>
<td align="right">3.1</td>
<td align="right">5.6</td>
<td align="right">2.4</td>
<td>virginica</td>
</tr>
<tr>
<td align="right">141</td>
<td align="right">6.9</td>
<td align="right">3.1</td>
<td align="right">5.1</td>
<td align="right">2.3</td>
<td>virginica</td>
</tr>
<tr>
<td align="right">142</td>
<td align="right">5.8</td>
<td align="right">2.7</td>
<td align="right">5.1</td>
<td align="right">1.9</td>
<td>virginica</td>
</tr>
<tr>
<td align="right">143</td>
<td align="right">6.8</td>
<td align="right">3.2</td>
<td align="right">5.9</td>
<td align="right">2.3</td>
<td>virginica</td>
</tr>
<tr>
<td align="right">144</td>
<td align="right">6.7</td>
<td align="right">3.3</td>
<td align="right">5.7</td>
<td align="right">2.5</td>
<td>virginica</td>
</tr>
<tr>
<td align="right">145</td>
<td align="right">6.7</td>
<td align="right">3.0</td>
<td align="right">5.2</td>
<td align="right">2.3</td>
<td>virginica</td>
</tr>
<tr>
<td align="right">146</td>
<td align="right">6.3</td>
<td align="right">2.5</td>
<td align="right">5.0</td>
<td align="right">1.9</td>
<td>virginica</td>
</tr>
<tr>
<td align="right">147</td>
<td align="right">6.5</td>
<td align="right">3.0</td>
<td align="right">5.2</td>
<td align="right">2.0</td>
<td>virginica</td>
</tr>
<tr>
<td align="right">148</td>
<td align="right">6.2</td>
<td align="right">3.4</td>
<td align="right">5.4</td>
<td align="right">2.3</td>
<td>virginica</td>
</tr>
<tr>
<td align="right">149</td>
<td align="right">5.9</td>
<td align="right">3.0</td>
<td align="right">5.1</td>
<td align="right">1.8</td>
<td>virginica</td>
</tr>
</tbody>
</table>
<p>150 rows × 5 columns </p>
<p><img src="https://d31ezp3r8jwmks.cloudfront.net/variants/6bTVLAHna7XM5NkrEHeYXuKT/d2e337a4f6900f8d0798c596eb0607a8e0c2fbddb6a7ab7afcd60009c119d4c7" alt="Hasil gambar untuk dataset iris penjelasan" style="zoom: 50%;" /></p>
<h6 id="memuat-data">Memuat data</h6>
<p>Pertama mari kita memuat dataset anggur yang diperlukan dari dataset scikit-learn.</p>
<pre><code>#Import scikit-learn dataset library
from sklearn import datasets

#Load dataset
iris = datasets.load_iris()
iris
</code></pre>

<h6 id="menjelajahi-data">Menjelajahi Data</h6>
<p>Anda dapat mencetak nama target dan fitur, untuk memastikan Anda memiliki dataset yang tepat, seperti:</p>
<pre><code class="python"># print the names of the 13 features

print( &quot;Features: &quot;, iris.feature_names)

# print the label type of iris(setosa, versicolor, virginica)

print (&quot;Labels: &quot;, iris.target_names)
</code></pre>

<p>output : </p>
<pre><code>Features:  ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']
Labels:  ['setosa' 'versicolor' 'virginica']
</code></pre>

<p>Merupakan ide bagus untuk selalu sedikit mengeksplorasi data Anda, sehingga Anda tahu apa yang sedang Anda kerjakan. Di sini, Anda dapat melihat lima baris pertama dataset dicetak, serta variabel target untuk seluruh dataset.</p>
<pre><code class="python"># print data(feature)shape
iris.data.shape
# print the iris data features (top 5 records)
print( iris.data[0:5])
# print the iris labels (0:setosa, 1:versicolor, 2:virginica)
print (iris.target)
</code></pre>

<p>output:</p>
<pre><code>[[5.1 3.5 1.4 0.2]
 [4.9 3.  1.4 0.2]
 [4.7 3.2 1.3 0.2]
 [4.6 3.1 1.5 0.2]
 [5.  3.6 1.4 0.2]]
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 2]
</code></pre>

<h4 id="memisahkan-data">Memisahkan Data</h4>
<p>Pertama, Anda memisahkan kolom menjadi variabel dependen dan independen (atau fitur dan label). Kemudian Anda membagi variabel-variabel tersebut ke dalam train dan set tes.</p>
<p><img src="https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1543836883/image_6_cfpjpr.png" alt="img" style="zoom: 80%;" /></p>
<pre><code class="python"># Import train_test_split function
from sklearn.model_selection import train_test_split

# Split dataset into training set and test set

X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3,random_state=109)
</code></pre>

<h4 id="pembuatan-model">Pembuatan Model</h4>
<p>Setelah pemisahan, Anda akan menghasilkan model hutan acak pada set pelatihan dan melakukan prediksi pada fitur set tes.</p>
<pre><code class="python">#Import Gaussian Naive Bayes model
from sklearn.naive_bayes import GaussianNB

#Create a Gaussian Classifier
gnb = GaussianNB()

#Train the model using the training sets
gnb.fit(X_train, y_train)

#Predict the response for test dataset
y_pred = gnb.predict(X_test)
</code></pre>

<h4 id="mengevaluasi-model">Mengevaluasi Model</h4>
<p>Setelah pembuatan model, periksa akurasi menggunakan nilai aktual dan prediksi.</p>
<pre><code class="python">#Import scikit-learn metrics module for accuracy calculation
from sklearn import metrics

# Model Accuracy, how often is the classifier correct?
print(&quot;Accuracy:&quot;,metrics.accuracy_score(y_test, y_pred))
</code></pre>

<p>output :</p>
<pre><code>Accuracy: 0.9555555555555556
</code></pre>

<h6 id="keuntungan">Keuntungan</h6>
<ul>
<li>Ini bukan hanya pendekatan sederhana tetapi juga metode prediksi yang cepat dan akurat.</li>
<li>Naive Bayes memiliki biaya perhitungan yang sangat rendah.</li>
<li>Secara efisien dapat bekerja pada dataset besar.</li>
<li>Berkinerja baik jika variabel respon diskrit dibandingkan dengan variabel kontinu.</li>
<li>Ini dapat digunakan dengan masalah prediksi beberapa kelas.</li>
<li>Ini juga berkinerja baik dalam masalah analitik teks.</li>
<li>Ketika asumsi independensi berlaku, classifier Naif Bayes berkinerja lebih baik dibandingkan dengan model lain seperti regresi logistik.</li>
</ul>
<h6 id="kekurangan">Kekurangan</h6>
<ul>
<li>Asumsi fitur independen. Dalam praktiknya, hampir tidak mungkin bahwa model akan mendapatkan seperangkat alat prediksi yang sepenuhnya independen.</li>
<li>Jika tidak ada tuple pelatihan dari kelas tertentu, ini menyebabkan probabilitas nol posterior. Dalam hal ini, model tidak dapat membuat prediksi. Masalah ini dikenal sebagai Zero Probability / Frequency Problem.</li>
</ul>
<h3 id="tugas-50"><u>TUGAS 5.0</u></h3>
<h4 id="w-knn-weighted-k-nearest-neighbour"><strong>W-KNN (Weighted K-Nearest Neighbour)</strong></h4>
<p>adalah versi modifikasi dari K-NN . Salah satu dari banyak masalah yang mempengaruhi kinerja algoritma kNN adalah pilihan hyperparameter k. Jika k terlalu kecil, algoritme akan lebih sensitif terhadap pencilan. Jika k terlalu besar, maka lingkungan tersebut mungkin memasukkan terlalu banyak poin dari kelas lain.
Masalah lainnya adalah pendekatan untuk menggabungkan label kelas. Metode paling sederhana adalah dengan mengambil suara terbanyak, tetapi ini bisa menjadi masalah jika tetangga terdekat sangat bervariasi dalam jarak mereka dan tetangga terdekat lebih andal menunjukkan kelas objek.</p>
<h5 id="intuisi"><strong>Intuisi:</strong></h5>
<p>Pertimbangkan set pelatihan berikut</p>
<p><img alt="img" src="file:///H:/KULIAH/SEMESTER%203/PENAMBANGAN%20DATA/Weighted%20K-NN%20-%20GeeksforGeeks_files/download10.png" /></p>
<p>Label merah menunjukkan poin kelas 0 dan label hijau menunjukkan poin kelas 1.
Pertimbangkan titik putih sebagai titik kueri (titik mana label kelas harus diprediksi)</p>
<p>Jika kita memberikan dataset di atas ke classifier berbasis kNN, maka classifier akan mendeklarasikan point query milik kelas 0. Namun dalam plot, jelas bahwa point lebih dekat ke class 1 point dibandingkan dengan class. 0 poin. Untuk mengatasi kerugian ini, kNN tertimbang digunakan. Dalam kNN tertimbang, titik k terdekat diberi bobot menggunakan fungsi yang disebut sebagai fungsi kernel. Intuisi di balik kNN tertimbang, adalah untuk memberikan bobot lebih ke titik-titik yang dekat dan lebih sedikit bobot ke titik-titik yang lebih jauh. Fungsi apa pun dapat digunakan sebagai fungsi kernel untuk pengklasifikasi knn tertimbang yang nilainya berkurang dengan meningkatnya jarak. Fungsi sederhana yang digunakan adalah fungsi jarak terbalik.</p>
<h5 id="algoritma"><strong>Algoritma</strong></h5>
<ul>
<li>Misalkan L = {(x i , y i ), i = 1,. . . , n} menjadi seperangkat pelatihan pengamatan x i dengan kelas yang diberikan y i dan membiarkan x menjadi pengamatan baru (titik kueri), yang label kelasnya y harus diprediksi.</li>
<li>Hitung d (x i , x) untuk i = 1,. . . , n, jarak antara titik kueri dan setiap titik lainnya dalam set pelatihan.</li>
<li>Pilih D '⊆ D, set k poin data pelatihan terdekat ke poin kueri</li>
<li>Memprediksi kelas titik kueri, menggunakan pemungutan suara berbobot jarak. V mewakili label kelas. Gunakan rumus berikut</li>
</ul>
<p><img alt="img" src="file:///H:/KULIAH/SEMESTER%203/PENAMBANGAN%20DATA/Weighted%20K-NN%20-%20GeeksforGeeks_files/Formula2.jpg" /></p>
<h5 id="implementasi"><strong>Implementasi:</strong></h5>
<p>Anggap 0 sebagai label untuk kelas 0 dan 1 sebagai label untuk kelas 1. Di bawah ini adalah penerapan algoritma weighted-kNN.</p>
<blockquote>
<p>Fungsi ini menemukan klasifikasi p menggunakan
    tertimbang k algoritma tetangga terdekat. Ini mengasumsikan hanya dua
    dua kelas dan mengembalikan 0 jika p milik kelas 0, yang lain
    1 (milik kelas 1)</p>
<p>Parameter -
     poin: Kamus poin pelatihan yang memiliki dua kunci - 0 dan 1
         Setiap kunci memiliki daftar poin data pelatihan milik itu</p>
<p>​     p: Sebuah tuple, data titik uji bentuk (x, y)</p>
<p>​     k: jumlah tetangga terdekat yang perlu dipertimbangkan, standarnya adalah 3</p>
</blockquote>
<pre><code class="python">import math  

def weightedkNN(points,p,k=3):    
    distance=[]  
    for group in points:  
        for feature in points[group]:  
           # Hitung jarak euclidean p dari titik pelatihan
            euclidean_distance = math.sqrt((feature[0]-p[0])**2 +(feature[1]-p[1])**2)  
            # Tambahkan tupel formulir (jarak, grup) dalam daftar jarak
            distance.append((euclidean_distance,group))  

    # Urutkan daftar jarak dalam urutan menaik
    # dan pilih jarak k pertama 
    distance = sorted(distance)[:k]  

    freq1 = 0 # jumlah tertimbang dari grup 0
    freq2 = 0 # jumlah tertimbang dari grup 1

    for d in distance: 
        if d[1] == 0: 
            freq1 += (1 / d[0]) 

        elif d[1] == 1:  
            freq2 += (1 /d[0]) 


    return 0 if freq1&gt;freq2 else 1
</code></pre>

<pre><code class="python">def main():  

    # Kamus poin pelatihan yang memiliki dua kunci - 0 dan 1
    # key 0 punya poin milik kelas 0
    # kunci 1 memiliki poin milik kelas 1
    points = {0:[(0, 4),(1, 4.9),(1.6, 5.4),(2.2, 6),(2.8, 7),(3.2, 8),(3.4, 9)],  
            1:[(1.8, 1),(2.2, 3),(3, 4),(4, 4.5),(5, 5),(6, 5.5)]}  

    #titik point p(x,y)  
    p = (2, 4)  

    #jumlah tetangga 
    k = 5

    print(&quot;The value classified to query point is: {}&quot;.format(weightedkNN(points,p,k)))  

if __name__ == '__main__':  
    main()  
</code></pre>

<p>output : </p>
<pre><code>The value classified to query point is: 1
</code></pre>

<h4 id="w-knn-dengan-dataset-iris">W-KNN dengan Dataset Iris</h4>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
      
        <a href=".." class="btn btn-neutral" title="Home"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href=".." style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
    </span>
</div>
    <script>var base_url = '..';</script>
    <script src="../js/theme.js" defer></script>
      <script src="../search/main.js" defer></script>

</body>
</html>
