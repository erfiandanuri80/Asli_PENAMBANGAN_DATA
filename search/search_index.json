{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"| Welcome to My Task Blog | Nama : RIO ERFIAN NIM : 180411100040 PENAMBANGAN DATA 5B Repository tugas kuliah penambangan data For full documentation visit github.com/erfiandanuri80/Asli_PENAMBANGAN_DATA . Pilih tab task untuk melihat kumpulan task atau klik disini \"Hello World\"","title":"Home"},{"location":"#welcome-to-my-task-blog","text":"Nama : RIO ERFIAN NIM : 180411100040 PENAMBANGAN DATA 5B Repository tugas kuliah penambangan data For full documentation visit github.com/erfiandanuri80/Asli_PENAMBANGAN_DATA . Pilih tab task untuk melihat kumpulan task atau klik disini \"Hello World\"","title":"| Welcome to My Task Blog |"},{"location":"task/","text":"Pusat Tugas Penambangan Data For full documentation visit github.com/erfiandanuri80/Asli_PENAMBANGAN_DATA . Pusat Tugas Penambangan Data TUGAS 1.0 Statistika Deskriptif Manfaat dari statistik deskriptif Ukuran statistik deskriptif 1. Ukuran pemusatan 2. Ukuran keragaman Code TUGAS 2.0 Mengukur Jarak Data Mengukur Jarak Tipe Numerik Menghitung Jarak Ordinal Menghitung jarak Binary Menghitung Jarak Campuran display TUGAS 3.0 Seleksi Fitur Mencari Entropy Gain Skor Keseluruhan Gain TUGAS 4.0 Naive Bayes Classifiers Alur Kerja Klasifikasi Teorema Bayes Pendekatan Pertama (Dalam hal fitur tunggal) Pendekatan Kedua (Dalam hal banyak fitur) Bangunan Classifier di Scikit-learn Klasifikasi Naif Bayes Mendefinisikan Dataset Memuat data Menjelajahi Data Memisahkan Data Pembuatan Model Mengevaluasi Model Keuntungan Kekurangan TUGAS 5.0 W-KNN (Weighted K-Nearest Neighbour) Intuisi: Algoritma Implementasi: Dataset iris : Meghitung jarak dengan sampel data setelah disorting didapati 5 k teratas: TUGAS 1.0 Statistika Deskriptif Statistik deskriptif merupakan proses analisis statistik yang fokus kepada manejemen, penyajian, dan klasifikasi data. Dengan proses ini, data yang disajikan akan menjadi lebih menarik lebih mudah dipahami, dan mampu memberikan makna lebih bagi pengguna data. Ada beberapa bentuk media yang biasa digunakan sebagai analisis deksriptif, diantaranya adalah tabel, grafik, diagram, infografis, dll. Tabel dan grafik adalah media yang biasa kita gunakan dalam menyajikan statistik deskriptif. Manfaat dari statistik deskriptif Memberikan gambaran dan deskripsi bagaimana informasi yang dimiliki data tersebut Statistik deskriptif haruslah mampu memberikan gambaran informasi apa saja yang bisa didapat secara dari data yang kita gunakan. Daripada hanya menggunakan angka-angka tanpa format yang baku, akan lebih menarik bila ditampilan dalam bentuk grafik dan tabel. Menjelaskan karakteristik sebuah data Statistik deskriptif juga memberikan karakteristik tentang data yang digunakan. Hal ini penting karena kondisi data yang digunakan akan memengaruhi seluruh analisis data yang kita lakukan. download Book1.csv terdahulu disini import pandas as pd from scipy import stats d=pd.read_csv('Book1.csv') beratbadan tinggibadan umur banyak saudara 0 67 157 22 5 1 76 165 21 2 2 84 156 22 2 3 51 167 22 2 4 93 192 21 5 5 90 171 30 4 6 93 187 16 2 7 77 163 26 4 8 71 153 28 3 9 54 167 18 5 10 78 196 18 5 11 85 159 22 4 12 72 192 25 3 13 88 164 20 3 14 87 172 25 3 15 86 198 28 3 16 91 177 29 1 17 66 174 30 4 18 72 164 19 1 19 70 183 16 4 20 86 170 25 2 21 50 199 26 1 22 59 185 30 2 23 69 198 18 5 24 58 184 26 4 25 62 168 19 5 26 54 188 24 3 27 90 187 29 1 28 63 197 19 3 29 85 172 23 2 ... ... ... ... ... 70 67 159 21 3 71 68 173 23 1 72 66 154 23 2 73 99 177 28 2 74 59 197 20 5 75 84 183 22 4 76 68 184 17 2 77 95 195 17 2 78 91 157 28 5 79 87 194 16 4 80 62 194 20 2 81 81 186 27 4 82 69 175 29 2 83 91 174 29 4 84 65 178 21 2 85 61 156 19 1 86 83 156 19 4 87 93 185 18 4 88 63 187 28 5 89 79 184 17 5 90 86 157 23 5 91 86 172 26 4 92 57 196 28 3 93 52 200 15 5 94 64 162 18 3 95 57 168 16 1 96 88 171 18 4 97 58 160 23 1 98 99 155 29 5 99 57 168 18 1 mendescribe kan kolom : df=pd.read_csv('Book1.csv',usecols=[\"beratbadan\",\"tinggibadan\",\"umur\",\"banyaksaudara\"]) df.describe() beratbadan tinggibadan umur banyaksaudara count 100.000000 100.000000 100.000000 100.000000 mean 73.190000 175.070000 22.390000 3.040000 std 13.813838 14.286573 4.483178 1.434918 min 50.000000 150.000000 15.000000 1.000000 25% 61.750000 163.750000 18.750000 2.000000 50% 71.000000 174.000000 22.000000 3.000000 75% 86.000000 186.250000 26.250000 4.000000 max 99.000000 200.000000 30.000000 5.000000 Ukuran statistik deskriptif Secara umum, ada 2 jenis pengukuran statistik deskriptif. 1. Ukuran pemusatan Ukuran pemusatan adalah metode paling lazim yang digunakan dalam analisis deskriptif. Metode ini fokus untuk menggambarkan kondisi data di titik pusat. Secara umum, kita bisa melihat bagaimana kondisi data dengan melihat dimana letak pusat data tersebut. Biasanya, pusat data sendiri akan berada pada nilai tengah, meskipun tida selalu demikian. Untuk membuktikan hal ini secara matematis maka pengukuran yang sering digunakan adalah mean, median, dan modus. Kita bahas satu per satu. $$ $$ Mean merupakan rata-rata dari sekumpulan data yang kita miliki. Formulanya sangat sederhana. Anda hanya perlu menjumlah nilai dari seluruh data yang dimiliki dan membaginya dengan jumlah data tersebut. print(\"rata-rata \",df['beratbadan'].mean()) Median adalah nilai tengah dari sebuah data. Bila kita memiliki sekumpulan data, kita bisa mengurutkan data tersebut dari nilai terkecil hingga terbesar. Jika kita memiliki jumlah data ganjil, maka nilai tengah data tersebut akan langsung menjadi median. Namun bila kita memiliki data genap, kita perlu menemukan nilai rata-rata dari nilai tengah data tersebut. print(\"median \",df['beratbadan'].quantile(0.5)) Modus adalah nilai yang paling sering muncul dalam sekelompok data. Kita hanya perlu melihat nilai mana yang paling sering muncul dalam kelompok tersebut. Bila jumlah frekuensi setiap data sama, maka nilai modus tidak ada. mode=stats.mode(df) print(\"Nilai modus {} dengan jumlah {}\".format(mode.mode[0], mode.count[0])) 2. Ukuran keragaman Ukuran keragaman merupakan ukuran untuk menyajikan bagaimana sebaran dari data tersebut. Ukuran keragaman menunjukkan bagaimana kondisi sebuah data menyebar di kelompok data yang kita miliki. Hal ini memungkinkan kita untuk menganalisis seberapa jauh data-data tersebut tersebar dari ukuran pemusatannya. Bila sebaran datanya rendah, ini menunjukkan bahwa data tersebar tidak jauh dari pusatnya. Bila sebarannya jauh ini menunjukkan bahwa data tersebar jauh dari pusatnya. Range Range atau rentang merupakan selisih dari nilai terbesar dan nilai terkecil yang kita miliki. Range merupkan hal yang paling sederhana dan paling mudah dimengerti dalam ukuran penyebaran. Range menunjukkan seberapa jauh sebaran dengan mengabaikan bentuk distribusinya. Quartiles Range Rentang Quartiles atau rentang kuartil merupakan ukuran penyebaran yang membagi data menjadi 4 bagian. Sesuai dengan namanya, kuartil membagi data menjadi 25 persen di setiap bagiannya. Ada 3 jenis nilai kuartil yang perlu kita tahu : Q1 atau kuartil bawah yang memuat 25 persen dari data dengan nilai terendah Q2 atau kuartil tengah, yang membagi data menjadi 2 bagian sama besar 50 persen terkecil dan 50 persen terbesar. Q2 juga memiliki nilai yang sama dengan median Q3 atau kuartil atas yang memuat 25 persen dari data dengan nilai tertinggi. print(\"Q1 \",df['beratbadan'].quantile(0.25)) print(\"Q2 \",df['beratbadan'].quantile(0.5)) print(\"Q3 \",df['beratbadan'].quantile(0.75)) Persentil Persentil merupakan ukuran penyebaran yang membagi data menjadi 100 bagian sama besar. Desil Desil merupakan ukuran penyebaran yang membagi data menjadi 10 bagian sama besar. Varians Varians merupakan ukuran seberapa jauh menyebar dari nilai rata-ratanya. Semakin kecil nilai varians, semakin dekat sebaran data dengan rata-rata. Semakin besar nilai varian, semakin besar sebaran data terhadap nilai rata-ratanya. print(\"Variansi \",\"{0:.2f}\".format(round(df['beratbadan'].var(),2))) Standar deviasi Standar deviasi merupakan ukuran lain dari sebaran data terhadap rata-ratanya. Bila anda menggunakan varians, maka nilai yang anda dapatkan sangatlah besar. Nilai ini tidak mampu menggambarkan bagaimana sebaran data yang sebenarnya terhadap rata-rata. Untuk mendapatkan nilai yang lebih mudah diinterpretasikan, standar deviasi adalah ukuran yang lebih tepat. Standar deviasi menghasilkan nilai yang lebih kecil dan mampu menjelaskan bagaimana sebaran data terhadap rata-rata. Standar deviasi disebut juga dengan simpangan baku. print(\"Standar Deviasi \",\"{0:.2f}\".format(round(df['beratbadan'].std(),2))) Skewness Skewness merupakan ukuran yang menunjukkan bagaimana kemencengan sebuah data terhadap rata-ratanya. Skewness juga bisa dikatakan sebagai ukuran ketidaksimetrisan sebuah data. Sk > 0 artinya kurva dikatakan menceng kanan (positif) Sk = 0 artinya kurva normal Sk < 0 artinya menceng kiri (negatif) print(\"kemencengan \" ,\"{0:.6f}\".format(round(df['beratbadan'].skew(),6))) Code import pandas as pd from scipy import stats df=pd.read_csv(\"Book1.csv\",usecols=[0]) print(\"jumlah data \",df['beratbadan'].count()) print(\"rata-rata \",df['beratbadan'].mean()) print(\"nila minimal \",df['beratbadan'].min()) print(\"Q1 \",df['beratbadan'].quantile(0.25)) print(\"Q2 \",df['beratbadan'].quantile(0.5)) print(\"Q3 \",df['beratbadan'].quantile(0.75)) print(\"Nilai Max \",df['beratbadan'].max()) mode=stats.mode(df) print(\"Nilai modus {} dengan jumlah {}\".format(mode.mode[0], mode.count[0])) print(\"kemencengan \" ,\"{0:.6f}\".format(round(df['beratbadan'].skew(),6))) print(\"Standar Deviasi \",\"{0:.2f}\".format(round(df['beratbadan'].std(),2))) print(\"Variansi \",\"{0:.2f}\".format(round(df['beratbadan'].var(),2))) hasil : jumlah data 100 rata-rata 73.19 nila minimal 50 Q1 61.75 Q2 71.0 Q3 86.0 Nilai Max 99 kemencengan 0.19 Nilai modus [72] dengan jumlah [5] kemencengan 0.185820 Standar Deviasi 13.81 Variansi 190.82 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ TUGAS 2.0 Mengukur Jarak Data Mengukur Jarak Tipe Numerik Salah satu tantangan dalam era ini dengan datatabase yang memiliki banyak tipe data. Mengukur jarak adalah komponen utama dalam algoritma clustering berbasis jarak. Alogritma seperit Algoritma Partisioning misal K-Mean, K-medoidm dan fuzzy c-mean dan rough clustering bergantung pada jarak untuk melakukan pengelompokkan Sebelum menjelaskan tentang beberapa macam ukuran jarak, kita mendefinisikan terlebih dahulu yaiut v1,v2v1,v2 menyatakandua vektor yang menyatakan v1=x1,x2,...,xn,v2=y1,y2,...,yn,v1=x1,x2,...,xn,v2=y1,y2,...,yn, dimana xi,yixi,yi disebut attribut. Ada beberapa ukuran similaritas datau ukuran jarak, diantaranya: Minkowski Distance Manhattan distance Euclidean distance Average Distance Weighted euclidean distance Chord distance Mahalanobis distance Cosine measure Pearson correlation from scipy import stats import pandas as pd df = pd.read_csv('bupa.csv') a=df.iloc[33:38] a out: a b c d e f 33 88 96 28 21 3 1 34 94 65 22 18 5 1 35 91 72 155 68 3 2 36 85 54 47 33 3 2 37 79 39 14 19 3 2 kolom kategori : binary=[5] ordinal=[4] num=[0,1,2,3] Menghitung Jarak Numerik (averagedistance) def chordDist(v1,v2,jnis): jmlh=0 normv1=0 normv2=0 for x in range (len(jnis)): normv1=normv1+(int(a.values.tolist()[v1][jnis[x]])**2) normv2=normv2+(int(a.values.tolist()[v2][jnis[x]])**2) jmlh=jmlh+(int(a.values.tolist()[v1][jnis[x]])*int(a.values.tolist()[v2][jnis[x]])) return ((2-(2*jmlh/(normv1*normv2)))**0.5) Menghitung Jarak Ordinal Nilai-nilai atribut ordinal memiliki urutan atau peringkat, namun besarnya antara nilai-nilai berturut-turut tidak diketahui. Contohnya tingkatan kecil, sedang, besar untuk atribut ukuran. Atribut ordinal juga dapat diperoleh dari diskritisasi atribut numerik dengan membuat rentang nilai ke dalam sejumlah kategori tertentu. Kategori-kategori ini disusun dalam peringkat. Yaitu, rentang atribut numerik dapat dipetakan ke atribut ordinal ff yang memiliki MfMf state. Misalnya, kisaran suhu atribut skala-skala (dalam Celcius)dapat diatur ke dalam status berikut: \u221230 hingga \u221210, \u221210 hingga 10, 10 hingga 30, masing-masing mewakili kategori suhu dingin, suhu sedang, dan suhu hangat. MM adalah jumlah keadaan yang dapat dilakukan oleh atribut ordinalmemiliki. State ini menentukan peringkat 1,...,Mf def ordDist(v1,v2,jnis): jmlh=0 for x in range (len(jnis)): z1=int(a.values.tolist()[v1][jnis[x]])-1 z2=int(a.values.tolist()[v2][jnis[x]])-1 jmlh=jmlh+chordDist(z1,z2,jnis) return (jmlh) Menghitung jarak Binary Mari kita lihat similaritas dan desimilirity untuk objek yang dijelaskan oleh atribut biner simetris atau asimetris. Aatribut biner hanya memiliki dua status: 0 dan 1 Contoh atribut perokok menggambarkan seorang pasien, misalnya, 1 menunjukkan bahwa pasien merokok, sedangkan 0 menunjukkan pasien tidak merokok. Memperlakukan atribut biner sebagai atribut numerik tidak diperkenankan. Oleh karena itu, metode khusus untuk data biner diperlukan untuk membedakan komputasi. Jadi, bagaimana kita bisa menghitung ketidaksamaan antara dua atribut biner? \u201dSatu pendekatan melibatkan penghitungan matriks ketidaksamaan dari data biner yang diberikan. Jika semua atribut biner dianggap memiliki bobot yang sama, kita memiliki tabel kontingensi 2\u00d722\u00d72 di mana qq adalah jumlah atribut yang sama dengan 1 untuk kedua objek ii dan jj, rr adalah jumlah atribut yang sama dengan 1 untuk objek ii tetapi 0 untuk objek jj, ss adalah jumlah atribut yang sama dengan 0 untuk objek ii tetapi 1 untuk objek jj, dan tt adalah jumlah atribut yang sama dengan 0 untuk kedua objek ii dan jj. Jumlah total atribut adalah pp, di mana p=q+r+s+t def binaryDist(v1,v2,jnis): q=0 r=0 s=0 t=0 for x in range (len(jnis)): if (int(a.values.tolist()[v1][jnis[x]]))==1 and (int(a.values.tolist()[v2][jnis[x]]))==1: q=q+1 elif (int(a.values.tolist()[v1][jnis[x]]))==1 and (int(a.values.tolist()[v2][jnis[x]]))==2: r=r+1 elif (int(a.values.tolist()[v1][jnis[x]]))==2 and (int(a.values.tolist()[v2][jnis[x]]))==1: s=s+1 else: t=t+1 return ((r+s)/(q+r+s+t)) Menghitung Jarak Campuran jumlah jarak setiap jenis type data def jarak(v1,v2): return ((chordDist(v1,v2,num)+ordDist(v1,v2,ordinal)+binaryDist(v1,v2,binary))/4) display from IPython.display import HTML, display import tabulate table=[ [\"Data\"]+[\"Jarak\"]+[\"Numeric\"]+[\"Ordinal\"]+[\"Binary\"], [\"v1-v2\"]+[\"{:.3f}\".format(jarak(0,1))]+[\"{:.3f}\".format(chordDist(0,1,num))]+[\"{:.3f}\".format(ordDist(0,1,ordinal))]+[\"{:.3f}\".format(binaryDist(0,1,binary))], [\"v1-v3\"]+[\"{:.3f}\".format(jarak(0,2))]+[\"{:.3f}\".format(chordDist(0,3,num))]+[\"{:.3f}\".format(ordDist(0,2,ordinal))]+[\"{:.3f}\".format(binaryDist(0,2,binary))], [\"v2-v3\"]+[\"{:.3f}\".format(jarak(1,2))]+[\"{:.3f}\".format(chordDist(1,2,num))]+[\"{:.3f}\".format(ordDist(1,2,ordinal))]+[\"{:.3f}\".format(binaryDist(1,2,binary))], [\"v2-v4\"]+[\"{:.3f}\".format(jarak(1,3))]+[\"{:.3f}\".format(chordDist(1,3,num))]+[\"{:.3f}\".format(ordDist(1,3,ordinal))]+[\"{:.3f}\".format(binaryDist(1,3,binary))], ] display(HTML(tabulate.tabulate(table, tablefmt='html'))) out : Data Jarak Numeric Ordinal Binary v1-v2 0.687 1.414 1.333 0.000 v1-v3 0.937 1.414 1.333 1.000 v2-v3 0.937 1.414 1.333 1.000 v2-v4 0.937 1.414 1.333 1.000 source : - task2.ipynb & bupa.csv https://github.com/erfiandanuri80/Asli_PENAMBANGAN_DATA/tree/master/asset TUGAS 3.0 Seleksi Fitur Seleksi fitur adalah teknik untuk memilih fitur penting dan relevan terhadap data dan mengurangi fitur yang tidak relevan. Seleksi fitur bertujuan untuk memilih fitur terbaik dari suatu kumpulan data fitur. Seleksi fitur bertujuan untuk memilih fitur terbaik dari suatu kumpulan data fitur. Tujuan dari penelitian ini adalah menerapkan metode Information Gain dalam sistem seleksi fitur untuk Permasalahan cuaca . Metode Information Gain adalah metode yang menggunakan teknik scoring untuk pembobotan sebuah fitur dengan menggunakan maksimal entropy. Fitur yang dipilih adalah fitur dengan nilai Information Gain yang lebih besar atau sama dengan nilai threshold tertentu. from pandas import * from IPython.display import HTML, display from tabulate import tabulate from math import log from sklearn.feature_selection import mutual_info_classif def table(df): display(HTML(tabulate(df, tablefmt='html', headers='keys', showindex=False))) df = read_csv('feature_selection.csv', sep=';') table(df) outlook temperature humidity windy play sunny hot high False no sunny hot high True no overcast hot high False yes rainy mild high False yes rainy cool normal False yes rainy cool normal True no overcast cool normal True yes sunny mild high False no sunny cool normal False yes rainy mild normal False yes sunny mild normal True yes overcast mild high True yes overcast hot normal False yes rainy mild high True no Mencari Entropy Untuk menghitung Information gain perlu dihitung dahulu nilai informasi dalam suatu bits dari suatu kumpulan obyek. Cara penghitungan dilakukan dengan menggunakan konsep entropi. Entropi menyatakan impurity suatu kumpulan obyek . Berikut merupakan definisi dari entropi suatu ruang sampel data (S): $$ E(T) = \\sum_{i=1}^n {-P_i\\log{P_i}} $$ def findEntropy(column): rawGroups = df.groupby(column) targetGroups = [[key, len(data), len(data)/df[column].size] for key,data in rawGroups] targetGroups = DataFrame(targetGroups, columns=['value', 'count', 'probability']) return sum([-x*log(x,2) for x in targetGroups['probability']]), targetGroups, rawGroups entropyTarget, groupTargets, _ = findEntropy('play') table(groupTargets) print('entropy target =', entropyTarget) value count probability no 5 0.357143 yes 9 0.642857 entropy target = 0.9402859586706309 Gain Gain adalah sebuah fiktur yang terdapat pada sebuah data , untuk menghitungnya contoh rumusnya : $$ \\operatorname{Gain}(T, X) = \\operatorname{Entropy}(T) - \\sum_{v\\in{T}} \\frac{T_{X,v}}{T} E(T_{X,v}) $$ def findGain(column): entropyOutlook, groupOutlooks, rawOutlooks = findEntropy(column) table(groupOutlooks) gain = entropyTarget-sum(len(data)/len(df)*sum(-x/len(data)*log(x/len(data),2) for x in data.groupby('play').size()) for key,data in rawOutlooks) print(\"gain of\",column,\"is\",gain) return gain value count probability overcast 4 0.285714 rainy 5 0.357143 sunny 5 0.357143 gain of outlook is 0.2467498197744391 value count probability cool 4 0.285714 hot 4 0.285714 mild 6 0.428571 gain of temperature is 0.029222565658954647 value count probability high 7 0.5 normal 7 0.5 gain of humidity is 0.15183550136234136 value count probability False 8 0.571429 True 6 0.428571 gain of windy is 0.04812703040826927 Skor Keseluruhan Gain table(DataFrame(gains, columns=[\"Feature\", \"Gain Score\"]).sort_values(\"Gain Score\")[::-1]) Feature Gain Score outlook 0.24675 humidity 0.151836 windy 0.048127 temperature 0.0292226 TUGAS 4.0 Naive Bayes Classifiers Pengklasifikasi Naif Bayes adalah kumpulan algoritma klasifikasi berdasarkan Teorema Bayes . Ini bukan algoritma tunggal tetapi keluarga algoritma di mana mereka semua berbagi prinsip yang sama, yaitu setiap pasangan fitur yang diklasifikasi tidak tergantung satu sama lain. Alur Kerja Klasifikasi Setiap kali Anda melakukan klasifikasi, langkah pertama adalah memahami masalah dan mengidentifikasi fitur dan label potensial. Fitur adalah karakteristik atau atribut yang memengaruhi hasil label. Misalnya, dalam hal penyaluran pinjaman, manajer bank mengidentifikasi pekerjaan, pendapatan, usia, lokasi nasabah, riwayat pinjaman sebelumnya, riwayat transaksi, dan skor kredit. Karakteristik ini dikenal sebagai fitur yang membantu model mengklasifikasikan pelanggan. Klasifikasi memiliki dua fase, fase pembelajaran, dan fase evaluasi. Pada fase pembelajaran, classifier melatih modelnya pada dataset yang diberikan dan pada fase evaluasi, ia menguji kinerja classifier. Kinerja dievaluasi berdasarkan berbagai parameter seperti akurasi, kesalahan, presisi, dan penarikan. Naive Bayes adalah teknik klasifikasi statistik berdasarkan Bayes Theorem. Ini adalah salah satu algoritma pembelajaran terawasi yang paling sederhana. Klasifikasi Naive Bayes adalah algoritma yang cepat, akurat dan andal. Klasifikasi Naif Bayes memiliki akurasi dan kecepatan tinggi pada dataset besar. Klasifikasi Naive Bayes mengasumsikan bahwa efek fitur tertentu di kelas tidak tergantung pada fitur lainnya. Misalnya, pemohon pinjaman diinginkan atau tidak tergantung pada pendapatannya, pinjaman sebelumnya dan riwayat transaksi, usia, dan lokasi. Sekalipun fitur-fitur ini saling bergantung, fitur-fitur ini masih dianggap independen. Asumsi ini menyederhanakan perhitungan, dan itu sebabnya dianggap naif. Asumsi ini disebut independensi kondisional kelas. Teorema Bayes Teorema Bayes menemukan probabilitas suatu peristiwa terjadi mengingat probabilitas peristiwa lain yang telah terjadi. Teorema Bayes dinyatakan secara matematis sebagai persamaan berikut: P (h): probabilitas hipotesis h menjadi benar (terlepas dari data). Ini dikenal sebagai probabilitas sebelumnya dari h. P (D): probabilitas data (terlepas dari hipotesis). Ini dikenal sebagai probabilitas sebelumnya. P (h | D): probabilitas hipotesis h diberikan data D. Ini dikenal sebagai probabilitas posterior. P (D | h): probabilitas data d mengingat bahwa hipotesis h adalah benar. Ini dikenal sebagai probabilitas posterior. Sekarang, sehubungan dengan dataset kami, kami dapat menerapkan teorema Bayes dengan cara berikut: di mana, y adalah variabel kelas dan X adalah vektor fitur dependen (ukuran n ) di mana: Hanya untuk menghapus, contoh vektor fitur dan variabel kelas yang sesuai dapat berupa: (rujuk baris pertama dataset) Pendekatan Pertama (Dalam hal fitur tunggal) Pengklasifikasi Naive Bayes menghitung probabilitas suatu peristiwa dalam langkah-langkah berikut: Langkah 1: Hitung probabilitas sebelumnya untuk label kelas yang diberikan Langkah 2: Temukan probabilitas Peluang dengan setiap atribut untuk setiap kelas Langkah 3: Masukkan nilai ini dalam Formula Bayes dan hitung probabilitas posterior. Langkah 4: Lihat kelas mana yang memiliki probabilitas lebih tinggi, mengingat input milik kelas probabilitas lebih tinggi. Untuk menyederhanakan perhitungan probabilitas sebelum dan posterior Anda dapat menggunakan tabel dua frekuensi dan kemungkinan. Kedua tabel ini akan membantu Anda menghitung probabilitas sebelum dan belakang. Tabel Frekuensi berisi kemunculan label untuk semua fitur. Ada dua tabel kemungkinan. Kemungkinan Tabel 1 menunjukkan probabilitas label sebelumnya dan Kemungkinan Tabel 2 menunjukkan probabilitas posterior. Pendekatan Kedua (Dalam hal banyak fitur) Bangunan Classifier di Scikit-learn Klasifikasi Naif Bayes Mendefinisikan Dataset Naif Bayes dengan Banyak Label Sampai sekarang Anda telah belajar klasifikasi Naif Bayes dengan label biner. Sekarang Anda akan belajar tentang klasifikasi beberapa kelas di Naif Bayes. Yang dikenal sebagai klasifikasi multinomial Naive Bayes. Misalnya, jika Anda ingin mengklasifikasikan artikel berita tentang teknologi, hiburan, politik, atau olahraga. Pada bagian pembuatan model, Anda dapat menggunakan dataset iris yang merupakan multivariat kumpulan data diperkenalkan oleh British statistik dan biologi Ronald Fisher pada tahun 1936 makalahnya Penggunaan beberapa pengukuran di masalah taksonomi sebagai contoh analisis diskriminan linier . Kadang-kadang disebut set data Iris Anderson karena Edgar Anderson mengumpulkan data untuk menghitung variasi morfologis bunga Iris dari tiga spesies terkait.Dua dari tiga spesies dikumpulkan di Semenanjung Gasp\u00e9 \"semuanya berasal dari padang rumput yang sama, dan dipetik pada hari yang sama dan diukur pada saat yang sama oleh orang yang sama dengan peralatan yang sama\". Dataset terdiri dari 50 sampel dari masing-masing dari tiga spesies Iris ( Iris setosa , Iris virginica dan Iris versicolor ). Empat fitur diukur dari masing-masing sampel: panjang dan lebar sepal dan kelopak , dalam sentimeter. Berdasarkan kombinasi keempat fitur ini, Fisher mengembangkan model diskriminan linier untuk membedakan spesies dari satu sama lain. Dataset tersedia di perpustakaan scikit-learn. dataset = pd.read_csv('iris.csv',sep=\",\") dataset output : sepal_length sepal_width petal_length petal_width species 0 5.1 3.5 1.4 0.2 setosa 1 4.9 3.0 1.4 0.2 setosa 2 4.7 3.2 1.3 0.2 setosa 3 4.6 3.1 1.5 0.2 setosa 4 5.0 3.6 1.4 0.2 setosa 5 5.4 3.9 1.7 0.4 setosa 6 4.6 3.4 1.4 0.3 setosa 7 5.0 3.4 1.5 0.2 setosa 8 4.4 2.9 1.4 0.2 setosa 9 4.9 3.1 1.5 0.1 setosa 10 5.4 3.7 1.5 0.2 setosa 11 4.8 3.4 1.6 0.2 setosa 12 4.8 3.0 1.4 0.1 setosa 13 4.3 3.0 1.1 0.1 setosa 14 5.8 4.0 1.2 0.2 setosa 15 5.7 4.4 1.5 0.4 setosa 16 5.4 3.9 1.3 0.4 setosa 17 5.1 3.5 1.4 0.3 setosa 18 5.7 3.8 1.7 0.3 setosa 19 5.1 3.8 1.5 0.3 setosa 20 5.4 3.4 1.7 0.2 setosa 21 5.1 3.7 1.5 0.4 setosa 22 4.6 3.6 1.0 0.2 setosa 23 5.1 3.3 1.7 0.5 setosa 24 4.8 3.4 1.9 0.2 setosa 25 5.0 3.0 1.6 0.2 setosa 26 5.0 3.4 1.6 0.4 setosa 27 5.2 3.5 1.5 0.2 setosa 28 5.2 3.4 1.4 0.2 setosa 29 4.7 3.2 1.6 0.2 setosa ... ... ... ... ... ... 120 6.9 3.2 5.7 2.3 virginica 121 5.6 2.8 4.9 2.0 virginica 122 7.7 2.8 6.7 2.0 virginica 123 6.3 2.7 4.9 1.8 virginica 124 6.7 3.3 5.7 2.1 virginica 125 7.2 3.2 6.0 1.8 virginica 126 6.2 2.8 4.8 1.8 virginica 127 6.1 3.0 4.9 1.8 virginica 128 6.4 2.8 5.6 2.1 virginica 129 7.2 3.0 5.8 1.6 virginica 130 7.4 2.8 6.1 1.9 virginica 131 7.9 3.8 6.4 2.0 virginica 132 6.4 2.8 5.6 2.2 virginica 133 6.3 2.8 5.1 1.5 virginica 134 6.1 2.6 5.6 1.4 virginica 135 7.7 3.0 6.1 2.3 virginica 136 6.3 3.4 5.6 2.4 virginica 137 6.4 3.1 5.5 1.8 virginica 138 6.0 3.0 4.8 1.8 virginica 139 6.9 3.1 5.4 2.1 virginica 140 6.7 3.1 5.6 2.4 virginica 141 6.9 3.1 5.1 2.3 virginica 142 5.8 2.7 5.1 1.9 virginica 143 6.8 3.2 5.9 2.3 virginica 144 6.7 3.3 5.7 2.5 virginica 145 6.7 3.0 5.2 2.3 virginica 146 6.3 2.5 5.0 1.9 virginica 147 6.5 3.0 5.2 2.0 virginica 148 6.2 3.4 5.4 2.3 virginica 149 5.9 3.0 5.1 1.8 virginica 150 rows \u00d7 5 columns Memuat data Pertama mari kita memuat dataset anggur yang diperlukan dari dataset scikit-learn. #Import scikit-learn dataset library from sklearn import datasets #Load dataset iris = datasets.load_iris() iris Menjelajahi Data Anda dapat mencetak nama target dan fitur, untuk memastikan Anda memiliki dataset yang tepat, seperti: # print the names of the 13 features print( \"Features: \", iris.feature_names) # print the label type of iris(setosa, versicolor, virginica) print (\"Labels: \", iris.target_names) output : Features: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'] Labels: ['setosa' 'versicolor' 'virginica'] Merupakan ide bagus untuk selalu sedikit mengeksplorasi data Anda, sehingga Anda tahu apa yang sedang Anda kerjakan. Di sini, Anda dapat melihat lima baris pertama dataset dicetak, serta variabel target untuk seluruh dataset. # print data(feature)shape iris.data.shape # print the iris data features (top 5 records) print( iris.data[0:5]) # print the iris labels (0:setosa, 1:versicolor, 2:virginica) print (iris.target) output: [[5.1 3.5 1.4 0.2] [4.9 3. 1.4 0.2] [4.7 3.2 1.3 0.2] [4.6 3.1 1.5 0.2] [5. 3.6 1.4 0.2]] [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2] Memisahkan Data Pertama, Anda memisahkan kolom menjadi variabel dependen dan independen (atau fitur dan label). Kemudian Anda membagi variabel-variabel tersebut ke dalam train dan set tes. # Import train_test_split function from sklearn.model_selection import train_test_split # Split dataset into training set and test set X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3,random_state=109) Pembuatan Model Setelah pemisahan, Anda akan menghasilkan model hutan acak pada set pelatihan dan melakukan prediksi pada fitur set tes. #Import Gaussian Naive Bayes model from sklearn.naive_bayes import GaussianNB #Create a Gaussian Classifier gnb = GaussianNB() #Train the model using the training sets gnb.fit(X_train, y_train) #Predict the response for test dataset y_pred = gnb.predict(X_test) Mengevaluasi Model Setelah pembuatan model, periksa akurasi menggunakan nilai aktual dan prediksi. #Import scikit-learn metrics module for accuracy calculation from sklearn import metrics # Model Accuracy, how often is the classifier correct? print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred)) output : Accuracy: 0.9555555555555556 Keuntungan Ini bukan hanya pendekatan sederhana tetapi juga metode prediksi yang cepat dan akurat. Naive Bayes memiliki biaya perhitungan yang sangat rendah. Secara efisien dapat bekerja pada dataset besar. Berkinerja baik jika variabel respon diskrit dibandingkan dengan variabel kontinu. Ini dapat digunakan dengan masalah prediksi beberapa kelas. Ini juga berkinerja baik dalam masalah analitik teks. Ketika asumsi independensi berlaku, classifier Naif Bayes berkinerja lebih baik dibandingkan dengan model lain seperti regresi logistik. Kekurangan Asumsi fitur independen. Dalam praktiknya, hampir tidak mungkin bahwa model akan mendapatkan seperangkat alat prediksi yang sepenuhnya independen. Jika tidak ada tuple pelatihan dari kelas tertentu, ini menyebabkan probabilitas nol posterior. Dalam hal ini, model tidak dapat membuat prediksi. Masalah ini dikenal sebagai Zero Probability / Frequency Problem. TUGAS 5.0 W-KNN (Weighted K-Nearest Neighbour) adalah versi modifikasi dari K-NN . Salah satu dari banyak masalah yang mempengaruhi kinerja algoritma kNN adalah pilihan hyperparameter k. Jika k terlalu kecil, algoritme akan lebih sensitif terhadap pencilan. Jika k terlalu besar, maka lingkungan tersebut mungkin memasukkan terlalu banyak poin dari kelas lain. Masalah lainnya adalah pendekatan untuk menggabungkan label kelas. Metode paling sederhana adalah dengan mengambil suara terbanyak, tetapi ini bisa menjadi masalah jika tetangga terdekat sangat bervariasi dalam jarak mereka dan tetangga terdekat lebih andal menunjukkan kelas objek. Intuisi: Pertimbangkan set pelatihan berikut Label merah menunjukkan poin kelas 0 dan label hijau menunjukkan poin kelas 1. Pertimbangkan titik putih sebagai titik kueri (titik mana label kelas harus diprediksi) Jika kita memberikan dataset di atas ke classifier berbasis kNN, maka classifier akan mendeklarasikan point query milik kelas 0. Namun dalam plot, jelas bahwa point lebih dekat ke class 1 point dibandingkan dengan class. 0 poin. Untuk mengatasi kerugian ini, kNN tertimbang digunakan. Dalam kNN tertimbang, titik k terdekat diberi bobot menggunakan fungsi yang disebut sebagai fungsi kernel. Intuisi di balik kNN tertimbang, adalah untuk memberikan bobot lebih ke titik-titik yang dekat dan lebih sedikit bobot ke titik-titik yang lebih jauh. Fungsi apa pun dapat digunakan sebagai fungsi kernel untuk pengklasifikasi knn tertimbang yang nilainya berkurang dengan meningkatnya jarak. Fungsi sederhana yang digunakan adalah fungsi jarak terbalik. Algoritma Misalkan L = {(x i , y i ), i = 1,. . . , n} menjadi seperangkat pelatihan pengamatan x i dengan kelas yang diberikan y i dan membiarkan x menjadi pengamatan baru (titik kueri), yang label kelasnya y harus diprediksi. Hitung d (x i , x) untuk i = 1,. . . , n, jarak antara titik kueri dan setiap titik lainnya dalam set pelatihan. Pilih D '\u2286 D, set k poin data pelatihan terdekat ke poin kueri Memprediksi kelas titik kueri, menggunakan pemungutan suara berbobot jarak. V mewakili label kelas. Gunakan rumus berikut Implementasi: Dataset iris : sepal_length sepal_width petal_length petal_width species Sampel data 7 3 4,8 2 versicolor Training data sepal_length as A sepal_width as B petal_length as C petal_width as D species 6,9 3,1 5,1 2,3 virginica 6,7 3 5 1,7 versicolor 6,9 3,1 4,9 1,5 versicolor 6,7 3 5,2 2,3 virginica 6,7 3,1 4,7 1,5 versicolor 6,5 3,2 5,1 2 virginica 6,9 3,1 5,4 2,1 virginica 7 3,2 4,7 1,4 versicolor 6,5 3 5,2 2 virginica 6,8 2,8 4,8 1,4 versicolor 6,8 3 5,5 2,1 virginica 6,5 2,8 4,6 1,5 versicolor 6,7 3,1 4,4 1,4 versicolor 6,3 2,7 4,9 1,8 virginica 6,6 3 4,4 1,4 versicolor 6,6 2,9 4,6 1,3 versicolor 6,4 2,7 5,3 1,9 virginica 6,2 2,8 4,8 1,8 virginica 6,4 3,2 4,5 1,5 versicolor 6,4 3,2 5,3 2,3 virginica 6,3 3,3 4,7 1,6 versicolor 6,5 3 5,5 1,8 virginica 6,3 2,5 5 1,9 virginica 6,1 3 4,9 1,8 virginica 6,3 2,8 5,1 1,5 virginica 6,7 3,1 5,6 2,4 virginica 6,4 3,1 5,5 1,8 virginica 6,9 3,2 5,7 2,3 virginica 6,3 2,5 4,9 1,5 versicolor 6,7 3,3 5,7 2,1 virginica 6 3 4,8 1,8 virginica 6,4 2,8 5,6 2,1 virginica 6,4 2,8 5,6 2,2 virginica 6,4 2,9 4,3 1,3 versicolor 6,3 2,9 5,6 1,8 virginica 6,1 2,9 4,7 1,4 versicolor 7,2 3 5,8 1,6 virginica 6,1 3 4,6 1,4 versicolor 7,1 3 5,9 2,1 virginica 6,7 3,3 5,7 2,5 virginica 6,2 3,4 5,4 2,3 virginica 5,9 3,2 4,8 1,8 versicolor 6,5 3 5,8 2,2 virginica 5,9 3 5,1 1,8 virginica 6 2,7 5,1 1,6 versicolor 6 2,9 4,5 1,5 versicolor 6,7 2,5 5,8 1,8 virginica 6,8 3,2 5,9 2,3 virginica 6,2 2,9 4,3 1,3 versicolor 6 3,4 4,5 1,6 versicolor 6,3 3,4 5,6 2,4 virginica 6,1 2,8 4,7 1,2 versicolor 7,2 3,2 6 1,8 virginica 6,2 2,2 4,5 1,5 versicolor 6,3 2,3 4,4 1,3 versicolor 5,8 2,7 5,1 1,9 virginica 5,8 2,7 5,1 1,9 virginica 5,8 2,8 5,1 2,4 virginica 5,9 3 4,2 1,5 versicolor 7,4 2,8 6,1 1,9 virginica 6 2,2 5 1,5 virginica 6,1 2,6 5,6 1,4 virginica 5,7 2,5 5 2 virginica 6,1 2,8 4 1,3 versicolor 5,6 2,8 4,9 2 virginica 7,7 3 6,1 2,3 virginica 6,3 3,3 6 2,5 virginica 5,6 3 4,5 1,5 versicolor 5,7 2,8 4,5 1,3 versicolor 7,2 3,6 6,1 2,5 virginica 7,3 2,9 6,3 1,8 virginica 5,7 2,9 4,2 1,3 versicolor 5,7 3 4,2 1,2 versicolor 5,7 2,8 4,1 1,3 versicolor 5,8 2,6 4 1,2 versicolor 5,4 3 4,5 1,5 versicolor 5,6 2,7 4,2 1,3 versicolor 5,6 3 4,1 1,3 versicolor 5,8 2,7 3,9 1,2 versicolor 5,8 2,7 4,1 1 versicolor 5,5 2,6 4,4 1,2 versicolor 6 2,2 4 1 versicolor 7,6 3 6,6 2,1 virginica 5,5 2,5 4 1,3 versicolor 5,6 2,5 3,9 1,1 versicolor 5,5 2,3 4 1,3 versicolor 5,6 2,9 3,6 1,3 versicolor 7,9 3,8 6,4 2 virginica 7,7 2,8 6,7 2 virginica 5,5 2,4 3,8 1,1 versicolor 5,2 2,7 3,9 1,4 versicolor 5,7 2,6 3,5 1 versicolor 7,7 3,8 6,7 2,2 virginica 5,5 2,4 3,7 1 versicolor 4,9 2,5 4,5 1,7 virginica 7,7 2,6 6,9 2,3 virginica 5 2 3,5 1 versicolor 5 2,3 3,3 1 versicolor 5,1 2,5 3 1,1 versicolor 4,9 2,4 3,3 1 versicolor 5,7 3,8 1,7 0,3 setosa 5,1 3,8 1,9 0,4 setosa 5,4 3,9 1,7 0,4 setosa 5,1 3,3 1,7 0,5 setosa 5,4 3,4 1,7 0,2 setosa 5,4 3,4 1,5 0,4 setosa 5 3,5 1,6 0,6 setosa 4,8 3,4 1,9 0,2 setosa 5 3,4 1,6 0,4 setosa 5,7 4,4 1,5 0,4 setosa 5,4 3,7 1,5 0,2 setosa 5 3 1,6 0,2 setosa 5,3 3,7 1,5 0,2 setosa 5,1 3,7 1,5 0,4 setosa 5,2 3,5 1,5 0,2 setosa 5,1 3,8 1,6 0,2 setosa 5,1 3,4 1,5 0,2 setosa 5,5 3,5 1,3 0,2 setosa 5,1 3,8 1,5 0,3 setosa 5,4 3,9 1,3 0,4 setosa 5,2 3,4 1,4 0,2 setosa 5 3,4 1,5 0,2 setosa 5,1 3,5 1,4 0,3 setosa 4,8 3,1 1,6 0,2 setosa 4,8 3,4 1,6 0,2 setosa 5,5 4,2 1,4 0,2 setosa 5,8 4 1,2 0,2 setosa 5,1 3,5 1,4 0,2 setosa 4,7 3,2 1,6 0,2 setosa 5 3,3 1,4 0,2 setosa 4,9 3,1 1,5 0,1 setosa 4,9 3,1 1,5 0,1 setosa 4,9 3,1 1,5 0,1 setosa 5,2 4,1 1,5 0,1 setosa 5 3,6 1,4 0,2 setosa 4,9 3 1,4 0,2 setosa 4,8 3 1,4 0,3 setosa 5 3,5 1,3 0,3 setosa 4,6 3,1 1,5 0,2 setosa 4,8 3 1,4 0,1 setosa 5 3,2 1,2 0,2 setosa 4,6 3,4 1,4 0,3 setosa 4,6 3,2 1,4 0,2 setosa 4,7 3,2 1,3 0,2 setosa 4,4 2,9 1,4 0,2 setosa 4,5 2,3 1,3 0,3 setosa 4,4 3 1,3 0,2 setosa 4,4 3,2 1,3 0,2 setosa 4,6 3,6 1 0,2 setosa 4,3 3 1,1 0,1 setosa Meghitung jarak dengan sampel data (A-S1)^2 (B-S2)^2 (C-S3)^2 (D-S4)^2 SQRT(D) 0,0 0,0 0,1 0,1 0,4 0,1 0,0 0,0 0,1 0,5 0,0 0,0 0,0 0,3 0,5 0,1 0,0 0,2 0,1 0,6 0,1 0,0 0,0 0,3 0,6 0,3 0,0 0,1 0,0 0,6 0,0 0,0 0,4 0,0 0,6 0,0 0,0 0,0 0,4 0,6 0,3 0,0 0,2 0,0 0,6 0,0 0,0 0,0 0,4 0,7 0,0 0,0 0,5 0,0 0,7 0,3 0,0 0,0 0,3 0,8 0,1 0,0 0,2 0,4 0,8 0,5 0,1 0,0 0,0 0,8 0,2 0,0 0,2 0,4 0,8 0,2 0,0 0,0 0,5 0,8 0,4 0,1 0,3 0,0 0,8 0,6 0,0 0,0 0,0 0,8 0,4 0,0 0,1 0,3 0,9 0,4 0,0 0,3 0,1 0,9 0,5 0,1 0,0 0,2 0,9 0,3 0,0 0,5 0,0 0,9 0,5 0,3 0,0 0,0 0,9 0,8 0,0 0,0 0,0 0,9 0,5 0,0 0,1 0,3 0,9 0,1 0,0 0,6 0,2 0,9 0,4 0,0 0,5 0,0 0,9 0,0 0,0 0,8 0,1 1,0 0,5 0,3 0,0 0,3 1,0 0,1 0,1 0,8 0,0 1,0 1,0 0,0 0,0 0,0 1,0 0,4 0,0 0,6 0,0 1,0 0,4 0,0 0,6 0,0 1,0 0,4 0,0 0,3 0,5 1,1 0,5 0,0 0,6 0,0 1,1 0,8 0,0 0,0 0,4 1,1 0,0 0,0 1,0 0,2 1,1 0,8 0,0 0,0 0,4 1,1 0,0 0,0 1,2 0,0 1,1 0,1 0,1 0,8 0,3 1,1 0,6 0,2 0,4 0,1 1,1 1,2 0,0 0,0 0,0 1,1 0,3 0,0 1,0 0,0 1,1 1,2 0,0 0,1 0,0 1,2 1,0 0,1 0,1 0,2 1,2 1,0 0,0 0,1 0,3 1,2 0,1 0,3 1,0 0,0 1,2 0,0 0,0 1,2 0,1 1,2 0,6 0,0 0,3 0,5 1,2 1,0 0,2 0,1 0,2 1,2 0,5 0,2 0,6 0,2 1,2 0,8 0,0 0,0 0,6 1,2 0,0 0,0 1,4 0,0 1,2 0,6 0,6 0,1 0,3 1,3 0,5 0,5 0,2 0,5 1,3 1,4 0,1 0,1 0,0 1,3 1,4 0,1 0,1 0,0 1,3 1,4 0,0 0,1 0,2 1,3 1,2 0,0 0,4 0,3 1,3 0,2 0,0 1,7 0,0 1,4 1,0 0,6 0,0 0,3 1,4 0,8 0,2 0,6 0,4 1,4 1,7 0,3 0,0 0,0 1,4 0,8 0,0 0,6 0,5 1,4 2,0 0,0 0,0 0,0 1,4 0,5 0,0 1,7 0,1 1,5 0,5 0,1 1,4 0,3 1,5 2,0 0,0 0,1 0,3 1,5 1,7 0,0 0,1 0,5 1,5 0,0 0,4 1,7 0,3 1,5 0,1 0,0 2,3 0,0 1,5 1,7 0,0 0,4 0,5 1,6 1,7 0,0 0,4 0,6 1,6 1,7 0,0 0,5 0,5 1,6 1,4 0,2 0,6 0,6 1,7 2,6 0,0 0,1 0,3 1,7 2,0 0,1 0,4 0,5 1,7 2,0 0,0 0,5 0,5 1,7 1,4 0,1 0,8 0,6 1,7 1,4 0,1 0,5 1,0 1,7 2,3 0,2 0,2 0,6 1,8 1,0 0,6 0,6 1,0 1,8 0,4 0,0 3,2 0,0 1,9 2,3 0,3 0,6 0,5 1,9 2,0 0,3 0,8 0,8 2,0 2,3 0,5 0,6 0,5 2,0 2,0 0,0 1,4 0,5 2,0 0,8 0,6 2,6 0,0 2,0 0,5 0,0 3,6 0,0 2,0 2,3 0,4 1,0 0,8 2,1 3,2 0,1 0,8 0,4 2,1 1,7 0,2 1,7 1,0 2,1 0,5 0,6 3,6 0,0 2,2 2,3 0,4 1,2 1,0 2,2 4,4 0,3 0,1 0,1 2,2 0,5 0,2 4,4 0,1 2,3 4,0 1,0 1,7 1,0 2,8 4,0 0,5 2,3 1,0 2,8 3,6 0,3 3,2 0,8 2,8 4,4 0,4 2,3 1,0 2,8 1,7 0,6 9,6 2,9 3,9 3,6 0,6 8,4 2,6 3,9 2,6 0,8 9,6 2,6 3,9 3,6 0,1 9,6 2,3 3,9 2,6 0,2 9,6 3,2 3,9 2,6 0,2 10,9 2,6 4,0 4,0 0,3 10,2 2,0 4,1 4,8 0,2 8,4 3,2 4,1 4,0 0,2 10,2 2,6 4,1 1,7 2,0 10,9 2,6 4,1 2,6 0,5 10,9 3,2 4,1 4,0 0,0 10,2 3,2 4,2 2,9 0,5 10,9 3,2 4,2 3,6 0,5 10,9 2,6 4,2 3,2 0,3 10,9 3,2 4,2 3,6 0,6 10,2 3,2 4,2 3,6 0,2 10,9 3,2 4,2 2,3 0,3 12,3 3,2 4,2 3,6 0,6 10,9 2,9 4,2 2,6 0,8 12,3 2,6 4,3 3,2 0,2 11,6 3,2 4,3 4,0 0,2 10,9 3,2 4,3 3,6 0,3 11,6 2,9 4,3 4,8 0,0 10,2 3,2 4,3 4,8 0,2 10,2 3,2 4,3 2,3 1,4 11,6 3,2 4,3 1,4 1,0 13,0 3,2 4,3 3,6 0,3 11,6 3,2 4,3 5,3 0,0 10,2 3,2 4,3 4,0 0,1 11,6 3,2 4,3 4,4 0,0 10,9 3,6 4,3 4,4 0,0 10,9 3,6 4,3 4,4 0,0 10,9 3,6 4,3 3,2 1,2 10,9 3,6 4,4 4,0 0,4 11,6 3,2 4,4 4,4 0,0 11,6 3,2 4,4 4,8 0,0 11,6 2,9 4,4 4,0 0,3 12,3 2,9 4,4 5,8 0,0 10,9 3,2 4,5 4,8 0,0 11,6 3,6 4,5 4,0 0,0 13,0 3,2 4,5 5,8 0,2 11,6 2,9 4,5 5,8 0,0 11,6 3,2 4,5 5,3 0,0 12,3 3,2 4,6 6,8 0,0 11,6 3,2 4,6 6,3 0,5 12,3 2,9 4,7 6,8 0,0 12,3 3,2 4,7 6,8 0,0 12,3 3,2 4,7 5,8 0,4 14,4 3,2 4,9 7,3 0,0 13,7 3,6 5,0 setelah disorting didapati 5 k teratas: sepal_width as B petal_length as C petal_width as D species (A-S1)^2 (B-S2)^2 (C-S3)^2 (D-S4)^2 SQRT(D) 3,1 5,1 2,3 virginica 0,0 0,0 0,1 0,1 0,4 3 5 1,7 versicolor 0,1 0,0 0,0 0,1 0,5 3,1 4,9 1,5 versicolor 0,0 0,0 0,0 0,3 0,5 3 5,2 2,3 virginica 0,1 0,0 0,2 0,1 0,6 3,1 4,7 1,5 versicolor 0,1 0,0 0,0 0,3 0,6 lalu hitung berat antar variasi dengan 1/jaraknya : D 1/D Setosa versicolor virginica 0,4 2,236068 0 0 2,236068 0,5 2,132007 0 2,132007 0 0,5 1,889822 0 1,889822 0 0,6 1,714986 0 0 1,714986 0,6 1,666667 0 1,666667 0 sum 0 5,688496 3,951054 setelah dihitung nilai terbesar adalah : class = versicolor link download sheet disini","title":"Task"},{"location":"task/#pusat-tugas-penambangan-data","text":"For full documentation visit github.com/erfiandanuri80/Asli_PENAMBANGAN_DATA . Pusat Tugas Penambangan Data TUGAS 1.0 Statistika Deskriptif Manfaat dari statistik deskriptif Ukuran statistik deskriptif 1. Ukuran pemusatan 2. Ukuran keragaman Code TUGAS 2.0 Mengukur Jarak Data Mengukur Jarak Tipe Numerik Menghitung Jarak Ordinal Menghitung jarak Binary Menghitung Jarak Campuran display TUGAS 3.0 Seleksi Fitur Mencari Entropy Gain Skor Keseluruhan Gain TUGAS 4.0 Naive Bayes Classifiers Alur Kerja Klasifikasi Teorema Bayes Pendekatan Pertama (Dalam hal fitur tunggal) Pendekatan Kedua (Dalam hal banyak fitur) Bangunan Classifier di Scikit-learn Klasifikasi Naif Bayes Mendefinisikan Dataset Memuat data Menjelajahi Data Memisahkan Data Pembuatan Model Mengevaluasi Model Keuntungan Kekurangan TUGAS 5.0 W-KNN (Weighted K-Nearest Neighbour) Intuisi: Algoritma Implementasi: Dataset iris : Meghitung jarak dengan sampel data setelah disorting didapati 5 k teratas:","title":"Pusat Tugas Penambangan Data"},{"location":"task/#tugas-10","text":"","title":"TUGAS 1.0"},{"location":"task/#statistika-deskriptif","text":"Statistik deskriptif merupakan proses analisis statistik yang fokus kepada manejemen, penyajian, dan klasifikasi data. Dengan proses ini, data yang disajikan akan menjadi lebih menarik lebih mudah dipahami, dan mampu memberikan makna lebih bagi pengguna data. Ada beberapa bentuk media yang biasa digunakan sebagai analisis deksriptif, diantaranya adalah tabel, grafik, diagram, infografis, dll. Tabel dan grafik adalah media yang biasa kita gunakan dalam menyajikan statistik deskriptif.","title":"Statistika Deskriptif"},{"location":"task/#manfaat-dari-statistik-deskriptif","text":"Memberikan gambaran dan deskripsi bagaimana informasi yang dimiliki data tersebut Statistik deskriptif haruslah mampu memberikan gambaran informasi apa saja yang bisa didapat secara dari data yang kita gunakan. Daripada hanya menggunakan angka-angka tanpa format yang baku, akan lebih menarik bila ditampilan dalam bentuk grafik dan tabel. Menjelaskan karakteristik sebuah data Statistik deskriptif juga memberikan karakteristik tentang data yang digunakan. Hal ini penting karena kondisi data yang digunakan akan memengaruhi seluruh analisis data yang kita lakukan. download Book1.csv terdahulu disini import pandas as pd from scipy import stats d=pd.read_csv('Book1.csv') beratbadan tinggibadan umur banyak saudara 0 67 157 22 5 1 76 165 21 2 2 84 156 22 2 3 51 167 22 2 4 93 192 21 5 5 90 171 30 4 6 93 187 16 2 7 77 163 26 4 8 71 153 28 3 9 54 167 18 5 10 78 196 18 5 11 85 159 22 4 12 72 192 25 3 13 88 164 20 3 14 87 172 25 3 15 86 198 28 3 16 91 177 29 1 17 66 174 30 4 18 72 164 19 1 19 70 183 16 4 20 86 170 25 2 21 50 199 26 1 22 59 185 30 2 23 69 198 18 5 24 58 184 26 4 25 62 168 19 5 26 54 188 24 3 27 90 187 29 1 28 63 197 19 3 29 85 172 23 2 ... ... ... ... ... 70 67 159 21 3 71 68 173 23 1 72 66 154 23 2 73 99 177 28 2 74 59 197 20 5 75 84 183 22 4 76 68 184 17 2 77 95 195 17 2 78 91 157 28 5 79 87 194 16 4 80 62 194 20 2 81 81 186 27 4 82 69 175 29 2 83 91 174 29 4 84 65 178 21 2 85 61 156 19 1 86 83 156 19 4 87 93 185 18 4 88 63 187 28 5 89 79 184 17 5 90 86 157 23 5 91 86 172 26 4 92 57 196 28 3 93 52 200 15 5 94 64 162 18 3 95 57 168 16 1 96 88 171 18 4 97 58 160 23 1 98 99 155 29 5 99 57 168 18 1 mendescribe kan kolom : df=pd.read_csv('Book1.csv',usecols=[\"beratbadan\",\"tinggibadan\",\"umur\",\"banyaksaudara\"]) df.describe() beratbadan tinggibadan umur banyaksaudara count 100.000000 100.000000 100.000000 100.000000 mean 73.190000 175.070000 22.390000 3.040000 std 13.813838 14.286573 4.483178 1.434918 min 50.000000 150.000000 15.000000 1.000000 25% 61.750000 163.750000 18.750000 2.000000 50% 71.000000 174.000000 22.000000 3.000000 75% 86.000000 186.250000 26.250000 4.000000 max 99.000000 200.000000 30.000000 5.000000","title":"Manfaat dari statistik deskriptif"},{"location":"task/#ukuran-statistik-deskriptif","text":"Secara umum, ada 2 jenis pengukuran statistik deskriptif.","title":"Ukuran statistik deskriptif"},{"location":"task/#1-ukuran-pemusatan","text":"Ukuran pemusatan adalah metode paling lazim yang digunakan dalam analisis deskriptif. Metode ini fokus untuk menggambarkan kondisi data di titik pusat. Secara umum, kita bisa melihat bagaimana kondisi data dengan melihat dimana letak pusat data tersebut. Biasanya, pusat data sendiri akan berada pada nilai tengah, meskipun tida selalu demikian. Untuk membuktikan hal ini secara matematis maka pengukuran yang sering digunakan adalah mean, median, dan modus. Kita bahas satu per satu. $$ $$ Mean merupakan rata-rata dari sekumpulan data yang kita miliki. Formulanya sangat sederhana. Anda hanya perlu menjumlah nilai dari seluruh data yang dimiliki dan membaginya dengan jumlah data tersebut. print(\"rata-rata \",df['beratbadan'].mean()) Median adalah nilai tengah dari sebuah data. Bila kita memiliki sekumpulan data, kita bisa mengurutkan data tersebut dari nilai terkecil hingga terbesar. Jika kita memiliki jumlah data ganjil, maka nilai tengah data tersebut akan langsung menjadi median. Namun bila kita memiliki data genap, kita perlu menemukan nilai rata-rata dari nilai tengah data tersebut. print(\"median \",df['beratbadan'].quantile(0.5)) Modus adalah nilai yang paling sering muncul dalam sekelompok data. Kita hanya perlu melihat nilai mana yang paling sering muncul dalam kelompok tersebut. Bila jumlah frekuensi setiap data sama, maka nilai modus tidak ada. mode=stats.mode(df) print(\"Nilai modus {} dengan jumlah {}\".format(mode.mode[0], mode.count[0]))","title":"1. Ukuran pemusatan"},{"location":"task/#2-ukuran-keragaman","text":"Ukuran keragaman merupakan ukuran untuk menyajikan bagaimana sebaran dari data tersebut. Ukuran keragaman menunjukkan bagaimana kondisi sebuah data menyebar di kelompok data yang kita miliki. Hal ini memungkinkan kita untuk menganalisis seberapa jauh data-data tersebut tersebar dari ukuran pemusatannya. Bila sebaran datanya rendah, ini menunjukkan bahwa data tersebar tidak jauh dari pusatnya. Bila sebarannya jauh ini menunjukkan bahwa data tersebar jauh dari pusatnya. Range Range atau rentang merupakan selisih dari nilai terbesar dan nilai terkecil yang kita miliki. Range merupkan hal yang paling sederhana dan paling mudah dimengerti dalam ukuran penyebaran. Range menunjukkan seberapa jauh sebaran dengan mengabaikan bentuk distribusinya. Quartiles Range Rentang Quartiles atau rentang kuartil merupakan ukuran penyebaran yang membagi data menjadi 4 bagian. Sesuai dengan namanya, kuartil membagi data menjadi 25 persen di setiap bagiannya. Ada 3 jenis nilai kuartil yang perlu kita tahu : Q1 atau kuartil bawah yang memuat 25 persen dari data dengan nilai terendah Q2 atau kuartil tengah, yang membagi data menjadi 2 bagian sama besar 50 persen terkecil dan 50 persen terbesar. Q2 juga memiliki nilai yang sama dengan median Q3 atau kuartil atas yang memuat 25 persen dari data dengan nilai tertinggi. print(\"Q1 \",df['beratbadan'].quantile(0.25)) print(\"Q2 \",df['beratbadan'].quantile(0.5)) print(\"Q3 \",df['beratbadan'].quantile(0.75)) Persentil Persentil merupakan ukuran penyebaran yang membagi data menjadi 100 bagian sama besar. Desil Desil merupakan ukuran penyebaran yang membagi data menjadi 10 bagian sama besar. Varians Varians merupakan ukuran seberapa jauh menyebar dari nilai rata-ratanya. Semakin kecil nilai varians, semakin dekat sebaran data dengan rata-rata. Semakin besar nilai varian, semakin besar sebaran data terhadap nilai rata-ratanya. print(\"Variansi \",\"{0:.2f}\".format(round(df['beratbadan'].var(),2))) Standar deviasi Standar deviasi merupakan ukuran lain dari sebaran data terhadap rata-ratanya. Bila anda menggunakan varians, maka nilai yang anda dapatkan sangatlah besar. Nilai ini tidak mampu menggambarkan bagaimana sebaran data yang sebenarnya terhadap rata-rata. Untuk mendapatkan nilai yang lebih mudah diinterpretasikan, standar deviasi adalah ukuran yang lebih tepat. Standar deviasi menghasilkan nilai yang lebih kecil dan mampu menjelaskan bagaimana sebaran data terhadap rata-rata. Standar deviasi disebut juga dengan simpangan baku. print(\"Standar Deviasi \",\"{0:.2f}\".format(round(df['beratbadan'].std(),2))) Skewness Skewness merupakan ukuran yang menunjukkan bagaimana kemencengan sebuah data terhadap rata-ratanya. Skewness juga bisa dikatakan sebagai ukuran ketidaksimetrisan sebuah data. Sk > 0 artinya kurva dikatakan menceng kanan (positif) Sk = 0 artinya kurva normal Sk < 0 artinya menceng kiri (negatif) print(\"kemencengan \" ,\"{0:.6f}\".format(round(df['beratbadan'].skew(),6)))","title":"2. Ukuran keragaman"},{"location":"task/#code","text":"import pandas as pd from scipy import stats df=pd.read_csv(\"Book1.csv\",usecols=[0]) print(\"jumlah data \",df['beratbadan'].count()) print(\"rata-rata \",df['beratbadan'].mean()) print(\"nila minimal \",df['beratbadan'].min()) print(\"Q1 \",df['beratbadan'].quantile(0.25)) print(\"Q2 \",df['beratbadan'].quantile(0.5)) print(\"Q3 \",df['beratbadan'].quantile(0.75)) print(\"Nilai Max \",df['beratbadan'].max()) mode=stats.mode(df) print(\"Nilai modus {} dengan jumlah {}\".format(mode.mode[0], mode.count[0])) print(\"kemencengan \" ,\"{0:.6f}\".format(round(df['beratbadan'].skew(),6))) print(\"Standar Deviasi \",\"{0:.2f}\".format(round(df['beratbadan'].std(),2))) print(\"Variansi \",\"{0:.2f}\".format(round(df['beratbadan'].var(),2))) hasil : jumlah data 100 rata-rata 73.19 nila minimal 50 Q1 61.75 Q2 71.0 Q3 86.0 Nilai Max 99 kemencengan 0.19 Nilai modus [72] dengan jumlah [5] kemencengan 0.185820 Standar Deviasi 13.81 Variansi 190.82 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~","title":"Code"},{"location":"task/#tugas-20","text":"","title":"TUGAS 2.0"},{"location":"task/#mengukur-jarak-data","text":"","title":"Mengukur Jarak Data"},{"location":"task/#mengukur-jarak-tipe-numerik","text":"Salah satu tantangan dalam era ini dengan datatabase yang memiliki banyak tipe data. Mengukur jarak adalah komponen utama dalam algoritma clustering berbasis jarak. Alogritma seperit Algoritma Partisioning misal K-Mean, K-medoidm dan fuzzy c-mean dan rough clustering bergantung pada jarak untuk melakukan pengelompokkan Sebelum menjelaskan tentang beberapa macam ukuran jarak, kita mendefinisikan terlebih dahulu yaiut v1,v2v1,v2 menyatakandua vektor yang menyatakan v1=x1,x2,...,xn,v2=y1,y2,...,yn,v1=x1,x2,...,xn,v2=y1,y2,...,yn, dimana xi,yixi,yi disebut attribut. Ada beberapa ukuran similaritas datau ukuran jarak, diantaranya: Minkowski Distance Manhattan distance Euclidean distance Average Distance Weighted euclidean distance Chord distance Mahalanobis distance Cosine measure Pearson correlation from scipy import stats import pandas as pd df = pd.read_csv('bupa.csv') a=df.iloc[33:38] a out: a b c d e f 33 88 96 28 21 3 1 34 94 65 22 18 5 1 35 91 72 155 68 3 2 36 85 54 47 33 3 2 37 79 39 14 19 3 2 kolom kategori : binary=[5] ordinal=[4] num=[0,1,2,3] Menghitung Jarak Numerik (averagedistance) def chordDist(v1,v2,jnis): jmlh=0 normv1=0 normv2=0 for x in range (len(jnis)): normv1=normv1+(int(a.values.tolist()[v1][jnis[x]])**2) normv2=normv2+(int(a.values.tolist()[v2][jnis[x]])**2) jmlh=jmlh+(int(a.values.tolist()[v1][jnis[x]])*int(a.values.tolist()[v2][jnis[x]])) return ((2-(2*jmlh/(normv1*normv2)))**0.5)","title":"Mengukur Jarak Tipe Numerik"},{"location":"task/#menghitung-jarak-ordinal","text":"Nilai-nilai atribut ordinal memiliki urutan atau peringkat, namun besarnya antara nilai-nilai berturut-turut tidak diketahui. Contohnya tingkatan kecil, sedang, besar untuk atribut ukuran. Atribut ordinal juga dapat diperoleh dari diskritisasi atribut numerik dengan membuat rentang nilai ke dalam sejumlah kategori tertentu. Kategori-kategori ini disusun dalam peringkat. Yaitu, rentang atribut numerik dapat dipetakan ke atribut ordinal ff yang memiliki MfMf state. Misalnya, kisaran suhu atribut skala-skala (dalam Celcius)dapat diatur ke dalam status berikut: \u221230 hingga \u221210, \u221210 hingga 10, 10 hingga 30, masing-masing mewakili kategori suhu dingin, suhu sedang, dan suhu hangat. MM adalah jumlah keadaan yang dapat dilakukan oleh atribut ordinalmemiliki. State ini menentukan peringkat 1,...,Mf def ordDist(v1,v2,jnis): jmlh=0 for x in range (len(jnis)): z1=int(a.values.tolist()[v1][jnis[x]])-1 z2=int(a.values.tolist()[v2][jnis[x]])-1 jmlh=jmlh+chordDist(z1,z2,jnis) return (jmlh)","title":"Menghitung Jarak Ordinal"},{"location":"task/#menghitung-jarak-binary","text":"Mari kita lihat similaritas dan desimilirity untuk objek yang dijelaskan oleh atribut biner simetris atau asimetris. Aatribut biner hanya memiliki dua status: 0 dan 1 Contoh atribut perokok menggambarkan seorang pasien, misalnya, 1 menunjukkan bahwa pasien merokok, sedangkan 0 menunjukkan pasien tidak merokok. Memperlakukan atribut biner sebagai atribut numerik tidak diperkenankan. Oleh karena itu, metode khusus untuk data biner diperlukan untuk membedakan komputasi. Jadi, bagaimana kita bisa menghitung ketidaksamaan antara dua atribut biner? \u201dSatu pendekatan melibatkan penghitungan matriks ketidaksamaan dari data biner yang diberikan. Jika semua atribut biner dianggap memiliki bobot yang sama, kita memiliki tabel kontingensi 2\u00d722\u00d72 di mana qq adalah jumlah atribut yang sama dengan 1 untuk kedua objek ii dan jj, rr adalah jumlah atribut yang sama dengan 1 untuk objek ii tetapi 0 untuk objek jj, ss adalah jumlah atribut yang sama dengan 0 untuk objek ii tetapi 1 untuk objek jj, dan tt adalah jumlah atribut yang sama dengan 0 untuk kedua objek ii dan jj. Jumlah total atribut adalah pp, di mana p=q+r+s+t def binaryDist(v1,v2,jnis): q=0 r=0 s=0 t=0 for x in range (len(jnis)): if (int(a.values.tolist()[v1][jnis[x]]))==1 and (int(a.values.tolist()[v2][jnis[x]]))==1: q=q+1 elif (int(a.values.tolist()[v1][jnis[x]]))==1 and (int(a.values.tolist()[v2][jnis[x]]))==2: r=r+1 elif (int(a.values.tolist()[v1][jnis[x]]))==2 and (int(a.values.tolist()[v2][jnis[x]]))==1: s=s+1 else: t=t+1 return ((r+s)/(q+r+s+t))","title":"Menghitung jarak Binary"},{"location":"task/#menghitung-jarak-campuran","text":"jumlah jarak setiap jenis type data def jarak(v1,v2): return ((chordDist(v1,v2,num)+ordDist(v1,v2,ordinal)+binaryDist(v1,v2,binary))/4)","title":"Menghitung Jarak Campuran"},{"location":"task/#display","text":"from IPython.display import HTML, display import tabulate table=[ [\"Data\"]+[\"Jarak\"]+[\"Numeric\"]+[\"Ordinal\"]+[\"Binary\"], [\"v1-v2\"]+[\"{:.3f}\".format(jarak(0,1))]+[\"{:.3f}\".format(chordDist(0,1,num))]+[\"{:.3f}\".format(ordDist(0,1,ordinal))]+[\"{:.3f}\".format(binaryDist(0,1,binary))], [\"v1-v3\"]+[\"{:.3f}\".format(jarak(0,2))]+[\"{:.3f}\".format(chordDist(0,3,num))]+[\"{:.3f}\".format(ordDist(0,2,ordinal))]+[\"{:.3f}\".format(binaryDist(0,2,binary))], [\"v2-v3\"]+[\"{:.3f}\".format(jarak(1,2))]+[\"{:.3f}\".format(chordDist(1,2,num))]+[\"{:.3f}\".format(ordDist(1,2,ordinal))]+[\"{:.3f}\".format(binaryDist(1,2,binary))], [\"v2-v4\"]+[\"{:.3f}\".format(jarak(1,3))]+[\"{:.3f}\".format(chordDist(1,3,num))]+[\"{:.3f}\".format(ordDist(1,3,ordinal))]+[\"{:.3f}\".format(binaryDist(1,3,binary))], ] display(HTML(tabulate.tabulate(table, tablefmt='html'))) out : Data Jarak Numeric Ordinal Binary v1-v2 0.687 1.414 1.333 0.000 v1-v3 0.937 1.414 1.333 1.000 v2-v3 0.937 1.414 1.333 1.000 v2-v4 0.937 1.414 1.333 1.000 source : - task2.ipynb & bupa.csv https://github.com/erfiandanuri80/Asli_PENAMBANGAN_DATA/tree/master/asset","title":"display"},{"location":"task/#tugas-30","text":"","title":"TUGAS 3.0"},{"location":"task/#seleksi-fitur","text":"Seleksi fitur adalah teknik untuk memilih fitur penting dan relevan terhadap data dan mengurangi fitur yang tidak relevan. Seleksi fitur bertujuan untuk memilih fitur terbaik dari suatu kumpulan data fitur. Seleksi fitur bertujuan untuk memilih fitur terbaik dari suatu kumpulan data fitur. Tujuan dari penelitian ini adalah menerapkan metode Information Gain dalam sistem seleksi fitur untuk Permasalahan cuaca . Metode Information Gain adalah metode yang menggunakan teknik scoring untuk pembobotan sebuah fitur dengan menggunakan maksimal entropy. Fitur yang dipilih adalah fitur dengan nilai Information Gain yang lebih besar atau sama dengan nilai threshold tertentu. from pandas import * from IPython.display import HTML, display from tabulate import tabulate from math import log from sklearn.feature_selection import mutual_info_classif def table(df): display(HTML(tabulate(df, tablefmt='html', headers='keys', showindex=False))) df = read_csv('feature_selection.csv', sep=';') table(df) outlook temperature humidity windy play sunny hot high False no sunny hot high True no overcast hot high False yes rainy mild high False yes rainy cool normal False yes rainy cool normal True no overcast cool normal True yes sunny mild high False no sunny cool normal False yes rainy mild normal False yes sunny mild normal True yes overcast mild high True yes overcast hot normal False yes rainy mild high True no","title":"Seleksi Fitur"},{"location":"task/#mencari-entropy","text":"Untuk menghitung Information gain perlu dihitung dahulu nilai informasi dalam suatu bits dari suatu kumpulan obyek. Cara penghitungan dilakukan dengan menggunakan konsep entropi. Entropi menyatakan impurity suatu kumpulan obyek . Berikut merupakan definisi dari entropi suatu ruang sampel data (S): $$ E(T) = \\sum_{i=1}^n {-P_i\\log{P_i}} $$ def findEntropy(column): rawGroups = df.groupby(column) targetGroups = [[key, len(data), len(data)/df[column].size] for key,data in rawGroups] targetGroups = DataFrame(targetGroups, columns=['value', 'count', 'probability']) return sum([-x*log(x,2) for x in targetGroups['probability']]), targetGroups, rawGroups entropyTarget, groupTargets, _ = findEntropy('play') table(groupTargets) print('entropy target =', entropyTarget) value count probability no 5 0.357143 yes 9 0.642857 entropy target = 0.9402859586706309","title":"Mencari Entropy"},{"location":"task/#gain","text":"Gain adalah sebuah fiktur yang terdapat pada sebuah data , untuk menghitungnya contoh rumusnya : $$ \\operatorname{Gain}(T, X) = \\operatorname{Entropy}(T) - \\sum_{v\\in{T}} \\frac{T_{X,v}}{T} E(T_{X,v}) $$ def findGain(column): entropyOutlook, groupOutlooks, rawOutlooks = findEntropy(column) table(groupOutlooks) gain = entropyTarget-sum(len(data)/len(df)*sum(-x/len(data)*log(x/len(data),2) for x in data.groupby('play').size()) for key,data in rawOutlooks) print(\"gain of\",column,\"is\",gain) return gain value count probability overcast 4 0.285714 rainy 5 0.357143 sunny 5 0.357143 gain of outlook is 0.2467498197744391 value count probability cool 4 0.285714 hot 4 0.285714 mild 6 0.428571 gain of temperature is 0.029222565658954647 value count probability high 7 0.5 normal 7 0.5 gain of humidity is 0.15183550136234136 value count probability False 8 0.571429 True 6 0.428571 gain of windy is 0.04812703040826927","title":"Gain"},{"location":"task/#skor-keseluruhan-gain","text":"table(DataFrame(gains, columns=[\"Feature\", \"Gain Score\"]).sort_values(\"Gain Score\")[::-1]) Feature Gain Score outlook 0.24675 humidity 0.151836 windy 0.048127 temperature 0.0292226","title":"Skor Keseluruhan Gain"},{"location":"task/#tugas-40","text":"","title":"TUGAS 4.0"},{"location":"task/#naive-bayes-classifiers","text":"Pengklasifikasi Naif Bayes adalah kumpulan algoritma klasifikasi berdasarkan Teorema Bayes . Ini bukan algoritma tunggal tetapi keluarga algoritma di mana mereka semua berbagi prinsip yang sama, yaitu setiap pasangan fitur yang diklasifikasi tidak tergantung satu sama lain.","title":"Naive Bayes Classifiers"},{"location":"task/#alur-kerja-klasifikasi","text":"Setiap kali Anda melakukan klasifikasi, langkah pertama adalah memahami masalah dan mengidentifikasi fitur dan label potensial. Fitur adalah karakteristik atau atribut yang memengaruhi hasil label. Misalnya, dalam hal penyaluran pinjaman, manajer bank mengidentifikasi pekerjaan, pendapatan, usia, lokasi nasabah, riwayat pinjaman sebelumnya, riwayat transaksi, dan skor kredit. Karakteristik ini dikenal sebagai fitur yang membantu model mengklasifikasikan pelanggan. Klasifikasi memiliki dua fase, fase pembelajaran, dan fase evaluasi. Pada fase pembelajaran, classifier melatih modelnya pada dataset yang diberikan dan pada fase evaluasi, ia menguji kinerja classifier. Kinerja dievaluasi berdasarkan berbagai parameter seperti akurasi, kesalahan, presisi, dan penarikan. Naive Bayes adalah teknik klasifikasi statistik berdasarkan Bayes Theorem. Ini adalah salah satu algoritma pembelajaran terawasi yang paling sederhana. Klasifikasi Naive Bayes adalah algoritma yang cepat, akurat dan andal. Klasifikasi Naif Bayes memiliki akurasi dan kecepatan tinggi pada dataset besar. Klasifikasi Naive Bayes mengasumsikan bahwa efek fitur tertentu di kelas tidak tergantung pada fitur lainnya. Misalnya, pemohon pinjaman diinginkan atau tidak tergantung pada pendapatannya, pinjaman sebelumnya dan riwayat transaksi, usia, dan lokasi. Sekalipun fitur-fitur ini saling bergantung, fitur-fitur ini masih dianggap independen. Asumsi ini menyederhanakan perhitungan, dan itu sebabnya dianggap naif. Asumsi ini disebut independensi kondisional kelas.","title":"Alur Kerja Klasifikasi"},{"location":"task/#teorema-bayes","text":"Teorema Bayes menemukan probabilitas suatu peristiwa terjadi mengingat probabilitas peristiwa lain yang telah terjadi. Teorema Bayes dinyatakan secara matematis sebagai persamaan berikut: P (h): probabilitas hipotesis h menjadi benar (terlepas dari data). Ini dikenal sebagai probabilitas sebelumnya dari h. P (D): probabilitas data (terlepas dari hipotesis). Ini dikenal sebagai probabilitas sebelumnya. P (h | D): probabilitas hipotesis h diberikan data D. Ini dikenal sebagai probabilitas posterior. P (D | h): probabilitas data d mengingat bahwa hipotesis h adalah benar. Ini dikenal sebagai probabilitas posterior. Sekarang, sehubungan dengan dataset kami, kami dapat menerapkan teorema Bayes dengan cara berikut: di mana, y adalah variabel kelas dan X adalah vektor fitur dependen (ukuran n ) di mana: Hanya untuk menghapus, contoh vektor fitur dan variabel kelas yang sesuai dapat berupa: (rujuk baris pertama dataset)","title":"Teorema Bayes"},{"location":"task/#pendekatan-pertama-dalam-hal-fitur-tunggal","text":"Pengklasifikasi Naive Bayes menghitung probabilitas suatu peristiwa dalam langkah-langkah berikut: Langkah 1: Hitung probabilitas sebelumnya untuk label kelas yang diberikan Langkah 2: Temukan probabilitas Peluang dengan setiap atribut untuk setiap kelas Langkah 3: Masukkan nilai ini dalam Formula Bayes dan hitung probabilitas posterior. Langkah 4: Lihat kelas mana yang memiliki probabilitas lebih tinggi, mengingat input milik kelas probabilitas lebih tinggi. Untuk menyederhanakan perhitungan probabilitas sebelum dan posterior Anda dapat menggunakan tabel dua frekuensi dan kemungkinan. Kedua tabel ini akan membantu Anda menghitung probabilitas sebelum dan belakang. Tabel Frekuensi berisi kemunculan label untuk semua fitur. Ada dua tabel kemungkinan. Kemungkinan Tabel 1 menunjukkan probabilitas label sebelumnya dan Kemungkinan Tabel 2 menunjukkan probabilitas posterior.","title":"Pendekatan Pertama (Dalam hal fitur tunggal)"},{"location":"task/#pendekatan-kedua-dalam-hal-banyak-fitur","text":"","title":"Pendekatan Kedua (Dalam hal banyak fitur)"},{"location":"task/#bangunan-classifier-di-scikit-learn","text":"","title":"Bangunan Classifier di Scikit-learn"},{"location":"task/#klasifikasi-naif-bayes","text":"","title":"Klasifikasi Naif Bayes"},{"location":"task/#mendefinisikan-dataset","text":"Naif Bayes dengan Banyak Label Sampai sekarang Anda telah belajar klasifikasi Naif Bayes dengan label biner. Sekarang Anda akan belajar tentang klasifikasi beberapa kelas di Naif Bayes. Yang dikenal sebagai klasifikasi multinomial Naive Bayes. Misalnya, jika Anda ingin mengklasifikasikan artikel berita tentang teknologi, hiburan, politik, atau olahraga. Pada bagian pembuatan model, Anda dapat menggunakan dataset iris yang merupakan multivariat kumpulan data diperkenalkan oleh British statistik dan biologi Ronald Fisher pada tahun 1936 makalahnya Penggunaan beberapa pengukuran di masalah taksonomi sebagai contoh analisis diskriminan linier . Kadang-kadang disebut set data Iris Anderson karena Edgar Anderson mengumpulkan data untuk menghitung variasi morfologis bunga Iris dari tiga spesies terkait.Dua dari tiga spesies dikumpulkan di Semenanjung Gasp\u00e9 \"semuanya berasal dari padang rumput yang sama, dan dipetik pada hari yang sama dan diukur pada saat yang sama oleh orang yang sama dengan peralatan yang sama\". Dataset terdiri dari 50 sampel dari masing-masing dari tiga spesies Iris ( Iris setosa , Iris virginica dan Iris versicolor ). Empat fitur diukur dari masing-masing sampel: panjang dan lebar sepal dan kelopak , dalam sentimeter. Berdasarkan kombinasi keempat fitur ini, Fisher mengembangkan model diskriminan linier untuk membedakan spesies dari satu sama lain. Dataset tersedia di perpustakaan scikit-learn. dataset = pd.read_csv('iris.csv',sep=\",\") dataset output : sepal_length sepal_width petal_length petal_width species 0 5.1 3.5 1.4 0.2 setosa 1 4.9 3.0 1.4 0.2 setosa 2 4.7 3.2 1.3 0.2 setosa 3 4.6 3.1 1.5 0.2 setosa 4 5.0 3.6 1.4 0.2 setosa 5 5.4 3.9 1.7 0.4 setosa 6 4.6 3.4 1.4 0.3 setosa 7 5.0 3.4 1.5 0.2 setosa 8 4.4 2.9 1.4 0.2 setosa 9 4.9 3.1 1.5 0.1 setosa 10 5.4 3.7 1.5 0.2 setosa 11 4.8 3.4 1.6 0.2 setosa 12 4.8 3.0 1.4 0.1 setosa 13 4.3 3.0 1.1 0.1 setosa 14 5.8 4.0 1.2 0.2 setosa 15 5.7 4.4 1.5 0.4 setosa 16 5.4 3.9 1.3 0.4 setosa 17 5.1 3.5 1.4 0.3 setosa 18 5.7 3.8 1.7 0.3 setosa 19 5.1 3.8 1.5 0.3 setosa 20 5.4 3.4 1.7 0.2 setosa 21 5.1 3.7 1.5 0.4 setosa 22 4.6 3.6 1.0 0.2 setosa 23 5.1 3.3 1.7 0.5 setosa 24 4.8 3.4 1.9 0.2 setosa 25 5.0 3.0 1.6 0.2 setosa 26 5.0 3.4 1.6 0.4 setosa 27 5.2 3.5 1.5 0.2 setosa 28 5.2 3.4 1.4 0.2 setosa 29 4.7 3.2 1.6 0.2 setosa ... ... ... ... ... ... 120 6.9 3.2 5.7 2.3 virginica 121 5.6 2.8 4.9 2.0 virginica 122 7.7 2.8 6.7 2.0 virginica 123 6.3 2.7 4.9 1.8 virginica 124 6.7 3.3 5.7 2.1 virginica 125 7.2 3.2 6.0 1.8 virginica 126 6.2 2.8 4.8 1.8 virginica 127 6.1 3.0 4.9 1.8 virginica 128 6.4 2.8 5.6 2.1 virginica 129 7.2 3.0 5.8 1.6 virginica 130 7.4 2.8 6.1 1.9 virginica 131 7.9 3.8 6.4 2.0 virginica 132 6.4 2.8 5.6 2.2 virginica 133 6.3 2.8 5.1 1.5 virginica 134 6.1 2.6 5.6 1.4 virginica 135 7.7 3.0 6.1 2.3 virginica 136 6.3 3.4 5.6 2.4 virginica 137 6.4 3.1 5.5 1.8 virginica 138 6.0 3.0 4.8 1.8 virginica 139 6.9 3.1 5.4 2.1 virginica 140 6.7 3.1 5.6 2.4 virginica 141 6.9 3.1 5.1 2.3 virginica 142 5.8 2.7 5.1 1.9 virginica 143 6.8 3.2 5.9 2.3 virginica 144 6.7 3.3 5.7 2.5 virginica 145 6.7 3.0 5.2 2.3 virginica 146 6.3 2.5 5.0 1.9 virginica 147 6.5 3.0 5.2 2.0 virginica 148 6.2 3.4 5.4 2.3 virginica 149 5.9 3.0 5.1 1.8 virginica 150 rows \u00d7 5 columns","title":"Mendefinisikan Dataset"},{"location":"task/#memuat-data","text":"Pertama mari kita memuat dataset anggur yang diperlukan dari dataset scikit-learn. #Import scikit-learn dataset library from sklearn import datasets #Load dataset iris = datasets.load_iris() iris","title":"Memuat data"},{"location":"task/#menjelajahi-data","text":"Anda dapat mencetak nama target dan fitur, untuk memastikan Anda memiliki dataset yang tepat, seperti: # print the names of the 13 features print( \"Features: \", iris.feature_names) # print the label type of iris(setosa, versicolor, virginica) print (\"Labels: \", iris.target_names) output : Features: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'] Labels: ['setosa' 'versicolor' 'virginica'] Merupakan ide bagus untuk selalu sedikit mengeksplorasi data Anda, sehingga Anda tahu apa yang sedang Anda kerjakan. Di sini, Anda dapat melihat lima baris pertama dataset dicetak, serta variabel target untuk seluruh dataset. # print data(feature)shape iris.data.shape # print the iris data features (top 5 records) print( iris.data[0:5]) # print the iris labels (0:setosa, 1:versicolor, 2:virginica) print (iris.target) output: [[5.1 3.5 1.4 0.2] [4.9 3. 1.4 0.2] [4.7 3.2 1.3 0.2] [4.6 3.1 1.5 0.2] [5. 3.6 1.4 0.2]] [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]","title":"Menjelajahi Data"},{"location":"task/#memisahkan-data","text":"Pertama, Anda memisahkan kolom menjadi variabel dependen dan independen (atau fitur dan label). Kemudian Anda membagi variabel-variabel tersebut ke dalam train dan set tes. # Import train_test_split function from sklearn.model_selection import train_test_split # Split dataset into training set and test set X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3,random_state=109)","title":"Memisahkan Data"},{"location":"task/#pembuatan-model","text":"Setelah pemisahan, Anda akan menghasilkan model hutan acak pada set pelatihan dan melakukan prediksi pada fitur set tes. #Import Gaussian Naive Bayes model from sklearn.naive_bayes import GaussianNB #Create a Gaussian Classifier gnb = GaussianNB() #Train the model using the training sets gnb.fit(X_train, y_train) #Predict the response for test dataset y_pred = gnb.predict(X_test)","title":"Pembuatan Model"},{"location":"task/#mengevaluasi-model","text":"Setelah pembuatan model, periksa akurasi menggunakan nilai aktual dan prediksi. #Import scikit-learn metrics module for accuracy calculation from sklearn import metrics # Model Accuracy, how often is the classifier correct? print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred)) output : Accuracy: 0.9555555555555556","title":"Mengevaluasi Model"},{"location":"task/#keuntungan","text":"Ini bukan hanya pendekatan sederhana tetapi juga metode prediksi yang cepat dan akurat. Naive Bayes memiliki biaya perhitungan yang sangat rendah. Secara efisien dapat bekerja pada dataset besar. Berkinerja baik jika variabel respon diskrit dibandingkan dengan variabel kontinu. Ini dapat digunakan dengan masalah prediksi beberapa kelas. Ini juga berkinerja baik dalam masalah analitik teks. Ketika asumsi independensi berlaku, classifier Naif Bayes berkinerja lebih baik dibandingkan dengan model lain seperti regresi logistik.","title":"Keuntungan"},{"location":"task/#kekurangan","text":"Asumsi fitur independen. Dalam praktiknya, hampir tidak mungkin bahwa model akan mendapatkan seperangkat alat prediksi yang sepenuhnya independen. Jika tidak ada tuple pelatihan dari kelas tertentu, ini menyebabkan probabilitas nol posterior. Dalam hal ini, model tidak dapat membuat prediksi. Masalah ini dikenal sebagai Zero Probability / Frequency Problem.","title":"Kekurangan"},{"location":"task/#tugas-50","text":"","title":"TUGAS 5.0"},{"location":"task/#w-knn-weighted-k-nearest-neighbour","text":"adalah versi modifikasi dari K-NN . Salah satu dari banyak masalah yang mempengaruhi kinerja algoritma kNN adalah pilihan hyperparameter k. Jika k terlalu kecil, algoritme akan lebih sensitif terhadap pencilan. Jika k terlalu besar, maka lingkungan tersebut mungkin memasukkan terlalu banyak poin dari kelas lain. Masalah lainnya adalah pendekatan untuk menggabungkan label kelas. Metode paling sederhana adalah dengan mengambil suara terbanyak, tetapi ini bisa menjadi masalah jika tetangga terdekat sangat bervariasi dalam jarak mereka dan tetangga terdekat lebih andal menunjukkan kelas objek.","title":"W-KNN (Weighted K-Nearest Neighbour)"},{"location":"task/#intuisi","text":"Pertimbangkan set pelatihan berikut Label merah menunjukkan poin kelas 0 dan label hijau menunjukkan poin kelas 1. Pertimbangkan titik putih sebagai titik kueri (titik mana label kelas harus diprediksi) Jika kita memberikan dataset di atas ke classifier berbasis kNN, maka classifier akan mendeklarasikan point query milik kelas 0. Namun dalam plot, jelas bahwa point lebih dekat ke class 1 point dibandingkan dengan class. 0 poin. Untuk mengatasi kerugian ini, kNN tertimbang digunakan. Dalam kNN tertimbang, titik k terdekat diberi bobot menggunakan fungsi yang disebut sebagai fungsi kernel. Intuisi di balik kNN tertimbang, adalah untuk memberikan bobot lebih ke titik-titik yang dekat dan lebih sedikit bobot ke titik-titik yang lebih jauh. Fungsi apa pun dapat digunakan sebagai fungsi kernel untuk pengklasifikasi knn tertimbang yang nilainya berkurang dengan meningkatnya jarak. Fungsi sederhana yang digunakan adalah fungsi jarak terbalik.","title":"Intuisi:"},{"location":"task/#algoritma","text":"Misalkan L = {(x i , y i ), i = 1,. . . , n} menjadi seperangkat pelatihan pengamatan x i dengan kelas yang diberikan y i dan membiarkan x menjadi pengamatan baru (titik kueri), yang label kelasnya y harus diprediksi. Hitung d (x i , x) untuk i = 1,. . . , n, jarak antara titik kueri dan setiap titik lainnya dalam set pelatihan. Pilih D '\u2286 D, set k poin data pelatihan terdekat ke poin kueri Memprediksi kelas titik kueri, menggunakan pemungutan suara berbobot jarak. V mewakili label kelas. Gunakan rumus berikut","title":"Algoritma"},{"location":"task/#implementasi","text":"","title":"Implementasi:"},{"location":"task/#dataset-iris","text":"sepal_length sepal_width petal_length petal_width species Sampel data 7 3 4,8 2 versicolor Training data sepal_length as A sepal_width as B petal_length as C petal_width as D species 6,9 3,1 5,1 2,3 virginica 6,7 3 5 1,7 versicolor 6,9 3,1 4,9 1,5 versicolor 6,7 3 5,2 2,3 virginica 6,7 3,1 4,7 1,5 versicolor 6,5 3,2 5,1 2 virginica 6,9 3,1 5,4 2,1 virginica 7 3,2 4,7 1,4 versicolor 6,5 3 5,2 2 virginica 6,8 2,8 4,8 1,4 versicolor 6,8 3 5,5 2,1 virginica 6,5 2,8 4,6 1,5 versicolor 6,7 3,1 4,4 1,4 versicolor 6,3 2,7 4,9 1,8 virginica 6,6 3 4,4 1,4 versicolor 6,6 2,9 4,6 1,3 versicolor 6,4 2,7 5,3 1,9 virginica 6,2 2,8 4,8 1,8 virginica 6,4 3,2 4,5 1,5 versicolor 6,4 3,2 5,3 2,3 virginica 6,3 3,3 4,7 1,6 versicolor 6,5 3 5,5 1,8 virginica 6,3 2,5 5 1,9 virginica 6,1 3 4,9 1,8 virginica 6,3 2,8 5,1 1,5 virginica 6,7 3,1 5,6 2,4 virginica 6,4 3,1 5,5 1,8 virginica 6,9 3,2 5,7 2,3 virginica 6,3 2,5 4,9 1,5 versicolor 6,7 3,3 5,7 2,1 virginica 6 3 4,8 1,8 virginica 6,4 2,8 5,6 2,1 virginica 6,4 2,8 5,6 2,2 virginica 6,4 2,9 4,3 1,3 versicolor 6,3 2,9 5,6 1,8 virginica 6,1 2,9 4,7 1,4 versicolor 7,2 3 5,8 1,6 virginica 6,1 3 4,6 1,4 versicolor 7,1 3 5,9 2,1 virginica 6,7 3,3 5,7 2,5 virginica 6,2 3,4 5,4 2,3 virginica 5,9 3,2 4,8 1,8 versicolor 6,5 3 5,8 2,2 virginica 5,9 3 5,1 1,8 virginica 6 2,7 5,1 1,6 versicolor 6 2,9 4,5 1,5 versicolor 6,7 2,5 5,8 1,8 virginica 6,8 3,2 5,9 2,3 virginica 6,2 2,9 4,3 1,3 versicolor 6 3,4 4,5 1,6 versicolor 6,3 3,4 5,6 2,4 virginica 6,1 2,8 4,7 1,2 versicolor 7,2 3,2 6 1,8 virginica 6,2 2,2 4,5 1,5 versicolor 6,3 2,3 4,4 1,3 versicolor 5,8 2,7 5,1 1,9 virginica 5,8 2,7 5,1 1,9 virginica 5,8 2,8 5,1 2,4 virginica 5,9 3 4,2 1,5 versicolor 7,4 2,8 6,1 1,9 virginica 6 2,2 5 1,5 virginica 6,1 2,6 5,6 1,4 virginica 5,7 2,5 5 2 virginica 6,1 2,8 4 1,3 versicolor 5,6 2,8 4,9 2 virginica 7,7 3 6,1 2,3 virginica 6,3 3,3 6 2,5 virginica 5,6 3 4,5 1,5 versicolor 5,7 2,8 4,5 1,3 versicolor 7,2 3,6 6,1 2,5 virginica 7,3 2,9 6,3 1,8 virginica 5,7 2,9 4,2 1,3 versicolor 5,7 3 4,2 1,2 versicolor 5,7 2,8 4,1 1,3 versicolor 5,8 2,6 4 1,2 versicolor 5,4 3 4,5 1,5 versicolor 5,6 2,7 4,2 1,3 versicolor 5,6 3 4,1 1,3 versicolor 5,8 2,7 3,9 1,2 versicolor 5,8 2,7 4,1 1 versicolor 5,5 2,6 4,4 1,2 versicolor 6 2,2 4 1 versicolor 7,6 3 6,6 2,1 virginica 5,5 2,5 4 1,3 versicolor 5,6 2,5 3,9 1,1 versicolor 5,5 2,3 4 1,3 versicolor 5,6 2,9 3,6 1,3 versicolor 7,9 3,8 6,4 2 virginica 7,7 2,8 6,7 2 virginica 5,5 2,4 3,8 1,1 versicolor 5,2 2,7 3,9 1,4 versicolor 5,7 2,6 3,5 1 versicolor 7,7 3,8 6,7 2,2 virginica 5,5 2,4 3,7 1 versicolor 4,9 2,5 4,5 1,7 virginica 7,7 2,6 6,9 2,3 virginica 5 2 3,5 1 versicolor 5 2,3 3,3 1 versicolor 5,1 2,5 3 1,1 versicolor 4,9 2,4 3,3 1 versicolor 5,7 3,8 1,7 0,3 setosa 5,1 3,8 1,9 0,4 setosa 5,4 3,9 1,7 0,4 setosa 5,1 3,3 1,7 0,5 setosa 5,4 3,4 1,7 0,2 setosa 5,4 3,4 1,5 0,4 setosa 5 3,5 1,6 0,6 setosa 4,8 3,4 1,9 0,2 setosa 5 3,4 1,6 0,4 setosa 5,7 4,4 1,5 0,4 setosa 5,4 3,7 1,5 0,2 setosa 5 3 1,6 0,2 setosa 5,3 3,7 1,5 0,2 setosa 5,1 3,7 1,5 0,4 setosa 5,2 3,5 1,5 0,2 setosa 5,1 3,8 1,6 0,2 setosa 5,1 3,4 1,5 0,2 setosa 5,5 3,5 1,3 0,2 setosa 5,1 3,8 1,5 0,3 setosa 5,4 3,9 1,3 0,4 setosa 5,2 3,4 1,4 0,2 setosa 5 3,4 1,5 0,2 setosa 5,1 3,5 1,4 0,3 setosa 4,8 3,1 1,6 0,2 setosa 4,8 3,4 1,6 0,2 setosa 5,5 4,2 1,4 0,2 setosa 5,8 4 1,2 0,2 setosa 5,1 3,5 1,4 0,2 setosa 4,7 3,2 1,6 0,2 setosa 5 3,3 1,4 0,2 setosa 4,9 3,1 1,5 0,1 setosa 4,9 3,1 1,5 0,1 setosa 4,9 3,1 1,5 0,1 setosa 5,2 4,1 1,5 0,1 setosa 5 3,6 1,4 0,2 setosa 4,9 3 1,4 0,2 setosa 4,8 3 1,4 0,3 setosa 5 3,5 1,3 0,3 setosa 4,6 3,1 1,5 0,2 setosa 4,8 3 1,4 0,1 setosa 5 3,2 1,2 0,2 setosa 4,6 3,4 1,4 0,3 setosa 4,6 3,2 1,4 0,2 setosa 4,7 3,2 1,3 0,2 setosa 4,4 2,9 1,4 0,2 setosa 4,5 2,3 1,3 0,3 setosa 4,4 3 1,3 0,2 setosa 4,4 3,2 1,3 0,2 setosa 4,6 3,6 1 0,2 setosa 4,3 3 1,1 0,1 setosa","title":"Dataset iris :"},{"location":"task/#meghitung-jarak-dengan-sampel-data","text":"(A-S1)^2 (B-S2)^2 (C-S3)^2 (D-S4)^2 SQRT(D) 0,0 0,0 0,1 0,1 0,4 0,1 0,0 0,0 0,1 0,5 0,0 0,0 0,0 0,3 0,5 0,1 0,0 0,2 0,1 0,6 0,1 0,0 0,0 0,3 0,6 0,3 0,0 0,1 0,0 0,6 0,0 0,0 0,4 0,0 0,6 0,0 0,0 0,0 0,4 0,6 0,3 0,0 0,2 0,0 0,6 0,0 0,0 0,0 0,4 0,7 0,0 0,0 0,5 0,0 0,7 0,3 0,0 0,0 0,3 0,8 0,1 0,0 0,2 0,4 0,8 0,5 0,1 0,0 0,0 0,8 0,2 0,0 0,2 0,4 0,8 0,2 0,0 0,0 0,5 0,8 0,4 0,1 0,3 0,0 0,8 0,6 0,0 0,0 0,0 0,8 0,4 0,0 0,1 0,3 0,9 0,4 0,0 0,3 0,1 0,9 0,5 0,1 0,0 0,2 0,9 0,3 0,0 0,5 0,0 0,9 0,5 0,3 0,0 0,0 0,9 0,8 0,0 0,0 0,0 0,9 0,5 0,0 0,1 0,3 0,9 0,1 0,0 0,6 0,2 0,9 0,4 0,0 0,5 0,0 0,9 0,0 0,0 0,8 0,1 1,0 0,5 0,3 0,0 0,3 1,0 0,1 0,1 0,8 0,0 1,0 1,0 0,0 0,0 0,0 1,0 0,4 0,0 0,6 0,0 1,0 0,4 0,0 0,6 0,0 1,0 0,4 0,0 0,3 0,5 1,1 0,5 0,0 0,6 0,0 1,1 0,8 0,0 0,0 0,4 1,1 0,0 0,0 1,0 0,2 1,1 0,8 0,0 0,0 0,4 1,1 0,0 0,0 1,2 0,0 1,1 0,1 0,1 0,8 0,3 1,1 0,6 0,2 0,4 0,1 1,1 1,2 0,0 0,0 0,0 1,1 0,3 0,0 1,0 0,0 1,1 1,2 0,0 0,1 0,0 1,2 1,0 0,1 0,1 0,2 1,2 1,0 0,0 0,1 0,3 1,2 0,1 0,3 1,0 0,0 1,2 0,0 0,0 1,2 0,1 1,2 0,6 0,0 0,3 0,5 1,2 1,0 0,2 0,1 0,2 1,2 0,5 0,2 0,6 0,2 1,2 0,8 0,0 0,0 0,6 1,2 0,0 0,0 1,4 0,0 1,2 0,6 0,6 0,1 0,3 1,3 0,5 0,5 0,2 0,5 1,3 1,4 0,1 0,1 0,0 1,3 1,4 0,1 0,1 0,0 1,3 1,4 0,0 0,1 0,2 1,3 1,2 0,0 0,4 0,3 1,3 0,2 0,0 1,7 0,0 1,4 1,0 0,6 0,0 0,3 1,4 0,8 0,2 0,6 0,4 1,4 1,7 0,3 0,0 0,0 1,4 0,8 0,0 0,6 0,5 1,4 2,0 0,0 0,0 0,0 1,4 0,5 0,0 1,7 0,1 1,5 0,5 0,1 1,4 0,3 1,5 2,0 0,0 0,1 0,3 1,5 1,7 0,0 0,1 0,5 1,5 0,0 0,4 1,7 0,3 1,5 0,1 0,0 2,3 0,0 1,5 1,7 0,0 0,4 0,5 1,6 1,7 0,0 0,4 0,6 1,6 1,7 0,0 0,5 0,5 1,6 1,4 0,2 0,6 0,6 1,7 2,6 0,0 0,1 0,3 1,7 2,0 0,1 0,4 0,5 1,7 2,0 0,0 0,5 0,5 1,7 1,4 0,1 0,8 0,6 1,7 1,4 0,1 0,5 1,0 1,7 2,3 0,2 0,2 0,6 1,8 1,0 0,6 0,6 1,0 1,8 0,4 0,0 3,2 0,0 1,9 2,3 0,3 0,6 0,5 1,9 2,0 0,3 0,8 0,8 2,0 2,3 0,5 0,6 0,5 2,0 2,0 0,0 1,4 0,5 2,0 0,8 0,6 2,6 0,0 2,0 0,5 0,0 3,6 0,0 2,0 2,3 0,4 1,0 0,8 2,1 3,2 0,1 0,8 0,4 2,1 1,7 0,2 1,7 1,0 2,1 0,5 0,6 3,6 0,0 2,2 2,3 0,4 1,2 1,0 2,2 4,4 0,3 0,1 0,1 2,2 0,5 0,2 4,4 0,1 2,3 4,0 1,0 1,7 1,0 2,8 4,0 0,5 2,3 1,0 2,8 3,6 0,3 3,2 0,8 2,8 4,4 0,4 2,3 1,0 2,8 1,7 0,6 9,6 2,9 3,9 3,6 0,6 8,4 2,6 3,9 2,6 0,8 9,6 2,6 3,9 3,6 0,1 9,6 2,3 3,9 2,6 0,2 9,6 3,2 3,9 2,6 0,2 10,9 2,6 4,0 4,0 0,3 10,2 2,0 4,1 4,8 0,2 8,4 3,2 4,1 4,0 0,2 10,2 2,6 4,1 1,7 2,0 10,9 2,6 4,1 2,6 0,5 10,9 3,2 4,1 4,0 0,0 10,2 3,2 4,2 2,9 0,5 10,9 3,2 4,2 3,6 0,5 10,9 2,6 4,2 3,2 0,3 10,9 3,2 4,2 3,6 0,6 10,2 3,2 4,2 3,6 0,2 10,9 3,2 4,2 2,3 0,3 12,3 3,2 4,2 3,6 0,6 10,9 2,9 4,2 2,6 0,8 12,3 2,6 4,3 3,2 0,2 11,6 3,2 4,3 4,0 0,2 10,9 3,2 4,3 3,6 0,3 11,6 2,9 4,3 4,8 0,0 10,2 3,2 4,3 4,8 0,2 10,2 3,2 4,3 2,3 1,4 11,6 3,2 4,3 1,4 1,0 13,0 3,2 4,3 3,6 0,3 11,6 3,2 4,3 5,3 0,0 10,2 3,2 4,3 4,0 0,1 11,6 3,2 4,3 4,4 0,0 10,9 3,6 4,3 4,4 0,0 10,9 3,6 4,3 4,4 0,0 10,9 3,6 4,3 3,2 1,2 10,9 3,6 4,4 4,0 0,4 11,6 3,2 4,4 4,4 0,0 11,6 3,2 4,4 4,8 0,0 11,6 2,9 4,4 4,0 0,3 12,3 2,9 4,4 5,8 0,0 10,9 3,2 4,5 4,8 0,0 11,6 3,6 4,5 4,0 0,0 13,0 3,2 4,5 5,8 0,2 11,6 2,9 4,5 5,8 0,0 11,6 3,2 4,5 5,3 0,0 12,3 3,2 4,6 6,8 0,0 11,6 3,2 4,6 6,3 0,5 12,3 2,9 4,7 6,8 0,0 12,3 3,2 4,7 6,8 0,0 12,3 3,2 4,7 5,8 0,4 14,4 3,2 4,9 7,3 0,0 13,7 3,6 5,0","title":"Meghitung jarak dengan sampel data"},{"location":"task/#setelah-disorting-didapati-5-k-teratas","text":"sepal_width as B petal_length as C petal_width as D species (A-S1)^2 (B-S2)^2 (C-S3)^2 (D-S4)^2 SQRT(D) 3,1 5,1 2,3 virginica 0,0 0,0 0,1 0,1 0,4 3 5 1,7 versicolor 0,1 0,0 0,0 0,1 0,5 3,1 4,9 1,5 versicolor 0,0 0,0 0,0 0,3 0,5 3 5,2 2,3 virginica 0,1 0,0 0,2 0,1 0,6 3,1 4,7 1,5 versicolor 0,1 0,0 0,0 0,3 0,6 lalu hitung berat antar variasi dengan 1/jaraknya : D 1/D Setosa versicolor virginica 0,4 2,236068 0 0 2,236068 0,5 2,132007 0 2,132007 0 0,5 1,889822 0 1,889822 0 0,6 1,714986 0 0 1,714986 0,6 1,666667 0 1,666667 0 sum 0 5,688496 3,951054 setelah dihitung nilai terbesar adalah : class = versicolor link download sheet disini","title":"setelah disorting didapati 5 k teratas:"}]}